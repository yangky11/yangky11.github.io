<!DOCTYPE HTML>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0B5LHBSZYY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0B5LHBSZYY');
  </script>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kaiyu Yang</title>
  
  <meta name="author" content="Kaiyu Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
  <link rel="manifest" href="images/site.webmanifest">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:1.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kaiyu Yang</name>
              </p>
              <p>
                I am a postdoctoral researcher at Caltech in the <a style="text-decoration: none" target="_blank" href="https://www.cms.caltech.edu/">Computing + Mathematical Sciences (CMS) Department</a>
                , working with <a style="text-decoration: none" target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>. 
                I received my Ph.D. from Princeton University, where I was advised by <a style="text-decoration: none" target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a> 
                and also worked with <a style="text-decoration: none" target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a> and <a style="text-decoration: none" target="_blank" href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>. 
              </p>

              <p style="text-align:center">
                杨凯峪 &nbsp/&nbsp
                kaiyuy [MASK] caltech [MASK] edu &nbsp/&nbsp
                <a target="_blank" href="data/CV___Kaiyu_Yang.pdf">CV</a> &nbsp/&nbsp
                <a target="_blank" href="data/Bio.txt">Bio</a> &nbsp/&nbsp
                <a target="_blank"href="https://scholar.google.com/citations?user=FciCu4EAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://github.com/yangky11">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Kaiyu Yang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Kaiyu Yang - Circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>News</heading>
		<ul style="list-style-type: none">
		  <li><img src="images/fire.png" height="25em" /> My talk at <a a target="_blank" href="https://software-research-lunch.github.io/">Stanford Software Research Lunch</a> on LeanDojo: <a target="_blank" href="https://www.youtube.com/watch?v=qvaR_VXB4fA&feature=youtu.be">Video</a>.</li>
		  <li><img src="images/fire.png" height="25em" /> We released <a target="_blank" href="https://github.com/lean-dojo/LeanCopilot">Lean Copilot</a> for LLMs to act as copilots for theorem proving in Lean: <a target="_blank" href="https://youtu.be/OxFcZU5ihBk">Video demo</a>.</li>
		  <li><img src="images/fire.png" height="25em" /> My talk at <a a target="_blank" href="https://lftcm2023.github.io/colloquium/index.html">Lean for the Curious Mathematician</a> on LLMs for theorem proving: <a target="_blank" href="https://www.youtube.com/watch?v=HEG_2dd4Jtw">Video</a>.</li>
		  <li><img src="images/fire.png" height="25em" /> I am co-organizing the <a target="_blank" href="https://mathai2023.github.io/">MATH-AI Workshop</a> and the <a target="_blank" href="https://machine-learning-for-theorem-proving.github.io/">Tutorial on ML for Theorem Proving</a> at NeurIPS 2023.</li>
		  <li><img src="images/fire.png" height="25em" /> We released <a target="_blank" href="https://leandojo.org/">LeanDojo</a>: tools, models, and benchmarks for machine learning to prove theorems in <a target="_blank" href="https://leanprover.github.io/">Lean</a>.</li>
		</ul>
            </td>
          </tr>
        </tbody></table>      
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>My research focuses on <em>Neurosymbolic AI</em>, which aims to make machine learning capable of symbolic reasoning. I have approached the goal from two angles: (1) applying machine learning to symbolic reasoning tasks, such as mathematical reasoning and theorem proving in formal logic or natural language; (2) introducing symbolic components into machine learning models to make them more interpretable, verifiable, and data-efficient.</p>
              <p>Currently, I'm working on <em>AI that can understand and reason about mathematics</em>. Mathematical reasoning is a critical milestone toward human-level intelligence, and it can potentially transform many important problems in science and engineering, such as solving PDEs and formal verification.</p>
              <!-- 
                <p>I also worked on constructing and analyzing machine learning datasets, focusing on fairness, privacy, and mitigating dataset bias. Collectively, they contribute to a future of machine learning we can trust, even in real-world, high-stake applications.</p>
              -->
            </td>
          </tr>
		
        </tbody></table>
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	  <tr onmouseout="sciglm_stop()" onmouseover="sciglm_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sciglm_image'>
                  <img src='images/sciglm-1.png' width="150">
                </div>
                <img src='images/sciglm-2.png' width="150">
              </div>
              <script type="text/javascript">
                function sciglm_start() {
                  document.getElementById('sciglm_image').style.opacity = "1";
                }

                function sciglm_stop() {
                  document.getElementById('sciglm_image').style.opacity = "0";
                }
                sciglm_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <papertitle>SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning</papertitle>
              <br />
	      <a target="_blank" href="#">Dan Zhang</a>, <a target="_blank" href="https://acbull.github.io/">Ziniu Hu</a>, <a target="_blank" href="#">Sining Zhoubian</a>, <a target="_blank" href="https://zxdu.xyz/">Zhengxiao Du</a>, <strong>Kaiyu Yang</strong>, <a target="_blank">Zihan Wang</a>, <a target="_blank" href="https://www.yisongyue.com/">Yisong Yue</a>, <a target="_blank" href="https://keg.cs.tsinghua.edu.cn/yuxiao/">Yuxiao Dong</a>, and <a target="_blank" href="https://keg.cs.tsinghua.edu.cn/jietang/">Jie Tang</a>
              <br />
              <em>In submission</em>, 2023
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2401.07950">arXiv</a>
	       /
	      <a target="_blank" href="https://github.com/THUDM/SciGLM">code</a>
              <p></p>
              <p>
                We curated SciInstruct, a diverse and high-quality dataset of college-level mathematics, physics, chemistry, and formal proofs. Using SciInstruct to finetune the ChatGLM family of LLMs, we introduce SciGLM, a suite of scientific language models for college-level mathematical/scientific reasoning. 
              </p>
            </td>
          </tr>
		
	  <tr onmouseout="lean_copilot_stop()" onmouseover="lean_copilot_start()" bgcolor="#ffffd0">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lean_copilot_image'>
                  <img src='images/lean_copilot-1.png' width="150">
                </div>
                <img src='images/lean_copilot-2.png' width="150">
              </div>
              <script type="text/javascript">
                function lean_copilot_start() {
                  document.getElementById('lean_copilot_image').style.opacity = "1";
                }

                function lean_copilot_stop() {
                  document.getElementById('lean_copilot_image').style.opacity = "0";
                }
                lean_copilot_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <papertitle>Towards Large Language Models as Copilots for Theorem Proving in Lean</papertitle>
              <br />
	      <a target="_blank" href="https://peiyang-song.github.io/">Peiyang Song</a>, <strong>Kaiyu Yang<sup>†</sup></strong>, and <a target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a><sup>†</sup>&nbsp;&nbsp;&nbsp;(<sup>†</sup> Equal advising)
              <br />
              <em>NeurIPS MATH-AI Workshop</em>, 2023
              <br />
              <a target="_blank" href="https://github.com/lean-dojo/LeanCopilot">code</a>
	       /
	      <a target="_blank" href="https://youtu.be/OxFcZU5ihBk">demo</a>
              <p></p>
              <p>
                We introduce a framework for running neural network inference directly in Lean. It enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users, including tools for suggesting proof steps and completing intermediate proof goals using LLMs.
              </p>
            </td>
          </tr>
	
          <tr onmouseout="leandojo_stop()" onmouseover="leandojo_start()" bgcolor="#ffffd0">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='leandojo_image'>
                  <img src='images/leandojo-1.jpg' width="150">
                </div>
                <img src='images/leandojo-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function leandojo_start() {
                  document.getElementById('leandojo_image').style.opacity = "1";
                }

                function leandojo_stop() {
                  document.getElementById('leandojo_image').style.opacity = "0";
                }
                leandojo_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://leandojo.org/">
                <papertitle>LeanDojo: Theorem Proving with Retrieval-Augmented Language Models</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, <a target="_blank" href="https://aidanswope.com/about">Aidan Swope</a>, <a target="_blank" href="https://minimario.github.io/">Alex Gu</a>, <a target="_blank" href="https://rchalamala.github.io/">Rahul Chalamala</a>, <a target="_blank" href="https://peiyang-song.github.io/">Peiyang Song</a>, <a target="_blank" href="https://billysx.github.io/">Shixing Yu</a>, <a target="_blank" href="https://www.linkedin.com/in/saad-godil-9728353/">Saad Godil</a>, <a target="_blank" href="https://www.linkedin.com/in/ryan-prenger-18797ba1/">Ryan Prenger</a>, and <a target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track</em>, 2023, <font color="red"><strong>Oral presentation</strong></font>
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2306.15626">arXiv</a>
	      /
              <a target="_blank" href="https://leandojo.org/">project</a>
	      /
	      <a target="_blank" href="https://github.com/orgs/lean-dojo">code</a>
	      /
	      <a target="_blank" href="https://www.youtube.com/watch?v=qvaR_VXB4fA&feature=youtu.be">talk</a>
	      /
	      <a target="_blank" href="data/LeanDojo_slides.pdf">slides</a>
	      /
	      <a target="_blank" href="https://www.marktechpost.com/2023/07/01/can-llms-generate-mathematical-proofs-that-can-be-rigorously-checked-meet-leandojo-an-open-source-ai-playground-with-toolkits-benchmarks-and-models-for-large-language-models-to-prove-formal-theore/">media</a>
              <p></p>
              <p>Can LLMs generate mathematical proofs that can be rigorously checked? We release LeanDojo: an open-source playground consisting of toolkits, benchmarks, and models for LLMs to prove formal theorems in the Lean proof assistant.</p>
            </td>
          </tr>	
		
          <tr>
            <td style="padding:5px;width:17%;vertical-align:middle">
		<video width="150" controls>
		  <source src="images/infinigen.mp4" type="video/mp4">
		  Your browser does not support HTML video.
		</video>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://infinigen.org/">
                <papertitle>Infinite Photorealistic Worlds using Procedural Generation</papertitle>
              </a>
              <br />
	      <a target="_blank" href="https://araistrick.github.io/">Alexander Raistrick</a>*, <a target="_blank" href=https://www.lahavlipson.com/"">Lahav Lipson</a>*, <a target="_blank" href="https://mazeyu.github.io/">Zeyu Ma</a>*, <a target="_blank" href="https://www.cs.princeton.edu/~lm5483/">Lingjie Mei</a>, <a target="_blank" href="https://www.cs.princeton.edu/~mingzhew/">Mingzhe Wang</a>, <a target="_blank" href="https://zuoym15.github.io/">Yiming Zuo</a>, <a target="_blank" href="https://kkayan.com/">Karhan Kayan</a>, <a target="_blank" href="https://hermera.github.io/">Hongyu Wen</a>, <a target="_blank" href="">Beining Han</a>, <a target="_blank" href="">Yihan Wang</a>, <a target="_blank" href="https://www.alejandronewell.com/">Alejandro Newell</a>, <a target="_blank" href="https://heilaw.github.io/">Hei Law</a>, <a target="_blank" href="https://imankgoyal.github.io/">Ankit Goyal</a>, <strong>Kaiyu Yang</strong>, and <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2306.09310">arXiv</a>
	      /
	      <a target="_blank" href="https://infinigen.org/">project</a>
	      /
              <a target="_blank" href="https://github.com/princeton-vl/infinigen">code</a>
              <p></p>
              <p>
                Data drives progress in computer vision. We introduce Infinigen: a generator of unlimited high-quality 3D data. 100% procedural, no external assets, no AI. Free and open source.
              </p>
            </td>
          </tr>	
		  
          <tr onmouseout="metaqnl_stop()" onmouseover="metaqnl_start()" bgcolor="#ffffd0">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='metaqnl_image'>
                  <img src='images/metaqnl-1.jpg' width="150">
                </div>
                <img src='images/metaqnl-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function metaqnl_start() {
                  document.getElementById('metaqnl_image').style.opacity = "1";
                }

                function metaqnl_stop() {
                  document.getElementById('metaqnl_image').style.opacity = "0";
                }
                metaqnl_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2111.12038">
                <papertitle>Learning Symbolic Rules for Reasoning in Quasi-Natural Language</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong> and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Transactions on Machine Learning Research (TMLR)</em>, 2023
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2111.12038">arXiv</a>
	      /
              <a target="_blank" href="https://github.com/princeton-vl/MetaQNL">code</a>
              <p></p>
              <p>
                We propose <em>MetaQNL</em>, a symbolic "Quasi-Natural" language that can express both formal logic and natural language. Instead of manually constructing MetaQNL rules, we propose <em>MetaInduce</em>: an algorithm for learning rules from data. 
              </p>
            </td>
          </tr>		
          
          
          <tr onmouseout="nlproofs_stop()" onmouseover="nlproofs_start()" bgcolor="#ffffd0">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nlproofs_image'>
                  <img src='images/nlproofs-1.jpg' width="150">
                </div>
                <img src='images/nlproofs-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function nlproofs_start() {
                  document.getElementById('nlproofs_image').style.opacity = "1";
                }

                function nlproofs_stop() {
                  document.getElementById('nlproofs_image').style.opacity = "0";
                }
                nlproofs_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2205.12443">
                <papertitle>Generating Natural Language Proofs with Verifier-Guided Search</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, and 
							<a target="_blank" href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>
              <br />
              <em>Empirical Methods in Natural Language Processing (EMNLP)</em>, 2022, <font color="red"><strong>Oral presentation</strong></font>
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2205.12443">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-nlp/NLProofS">code</a>
              /
              <a target="_blank" href="data/nlproofs_slides.pdf">slides</a>
              /
              <a target="_blank" href="data/nlproofs_poster.pdf">poster</a>
              <p></p>
              <p>
                We introduce NLProofS (<u>N</u>atural <u>L</u>anguage <u>P</u>roof <u>S</u>earch) for multi-step logical reasoning in natural language. Given a hypothesis and a set of supporting facts, it generates a proof tree indicating how to derive the hypothesis from supporting facts.
              </p>
            </td>
          </tr>

          <tr onmouseout="imagenet_privacy_stop()" onmouseover="imagenet_privacy_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='imagenet_privacy_image'>
                  <img src='images/imagenet_privacy-1.jpg' width="150">
                </div>
                <img src='images/imagenet_privacy-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function imagenet_privacy_start() {
                  document.getElementById('imagenet_privacy_image').style.opacity = "1";
                }

                function imagenet_privacy_stop() {
                  document.getElementById('imagenet_privacy_image').style.opacity = "0";
                }
                imagenet_privacy_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="http://image-net.org/face-obfuscation">
                <papertitle>A Study of Face Obfuscation in ImageNet</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.linkedin.com/in/jacqueline-yau-836b0a132">Jacqueline Yau</a>, 
              <a target="_blank" href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, and 
							<a target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a> 
              <br />
              <em>International Conference on Machine Learning (ICML)</em>, 2022
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2103.06191">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princetonvisualai/imagenet-face-obfuscation">code</a>
              /
              <a target="_blank" href="data/imagenet_privacy_slides.pdf">slides</a>
              /
              <a target="_blank" href="https://youtu.be/VnszYe8abRs?t=889">talk</a>
              /
              <a target="_blank" href="data/imagenet_face_obfuscation_icml_poster.pdf">poster</a>
              /
              <a target="_blank" href="http://image-net.org/face-obfuscation">project</a>
              /
              <a target="_blank" href="https://insights.princeton.edu/2022/01/tradeoff-privacy-algorithm-performance">media</a>
              <p></p>
              <p>We annotate human faces in ImageNet and obfuscate them for privacy protection. We show that face obfuscation does not hurt image classification and transfer learning.</p>
            </td>
          </tr>

          <tr onmouseout="attach_juxtapose_stop()" onmouseover="attach_juxtapose_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='attach_juxtapose_image'>
                  <img src='images/attach_juxtapose-1.jpg' width="150">
                </div>
                <img src='images/attach_juxtapose-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function attach_juxtapose_start() {
                  document.getElementById('attach_juxtapose_image').style.opacity = "1";
                }

                function attach_juxtapose_stop() {
                  document.getElementById('attach_juxtapose_image').style.opacity = "0";
                }
                attach_juxtapose_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2010.14568">
                <papertitle>Strongly Incremental Constituency Parsing with Graph Neural Networks</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong> and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2020
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2010.14568">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/attach-juxtapose-parser">code</a>
              /
              <a target="_blank" href="data/parsing_neurips_slides.pdf">slides</a>
              /
              <a target="_blank" href="https://crossminds.ai/video/strongly-incremental-constituency-parsing-with-graph-neural-networks-606fee03f43a7f2f827c13c3/">talk</a>
              /
              <a target="_blank" href="data/parsing_neurips_poster.pdf">poster</a>
              <p></p>
              <p>We propose a novel transition-based constituency parser named <em>Attach-Juxtapose</em>, inspired by how humans perform parsing.</p>
            </td>
          </tr>
					
          <tr>
            <td style="padding:5px;width:17%;vertical-align:middle">
              <img src='images/rel3d.gif' width="150">
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2012.01634">
                <papertitle>Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://imankgoyal.github.io/">Ankit Goyal</a>, 
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="http://www-personal.umich.edu/~ydawei/">Dawei Yang</a>, and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2020, <font color="red"><strong>Spotlight presentation</strong></font>
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2012.01634">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/Rel3D">code</a>
              <p></p>
              <p>We propose <em>Minimally Contrastive Data Collection</em>: a novel crowdsourcing method for reducing dataset bias. And we use it to construct Rel3D—the first large-scale, human-annotated dataset for grounding spatial relations in 3D.</p>
            </td>
          </tr>

          <tr onmouseout="imagenet_fairness_stop()" onmouseover="imagenet_fairness_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='imagenet_fairness_image'>
                  <img src='images/imagenet_fairness-1.jpg' width="150">
                </div>  
                <img src='images/imagenet_fairness-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function imagenet_fairness_start() {
                  document.getElementById('imagenet_fairness_image').style.opacity = "1";
                }

                function imagenet_fairness_stop() {
                  document.getElementById('imagenet_fairness_image').style.opacity = "0";
                }
                imagenet_fairness_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1912.07726">
                <papertitle>Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.linkedin.com/in/klint-qinami/">Klint Qinami</a>, 
              <a target="_blank" href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, and 
							<a target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a>
              <br />
              <em>Conference on Fairness, Accountability, and Transparency (FAT*)</em>, 2020
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1912.07726">arXiv</a>
              /
              <a target="_blank" href="data/imagenet_debias_fat_slides.pdf">slides</a>
              /
              <a target="_blank" href="https://www.youtube.com/watch?v=MLaqxuNtgAo">talk</a>
              /
              <a target="_blank" href="http://image-net.org/update-sep-17-2019">blog</a>
              /
              <a target="_blank" href="https://www.wired.com/story/ai-biased-how-scientists-trying-fix">media</a>
              <p></p>
              <p>We reveal and mitigate fairness issues of ImageNet, filtering its concept vocabulary and balancing its representation of various demographic groups in images.</p>
            </td>
          </tr>

          <tr onmouseout="coqgym_stop()" onmouseover="coqgym_start()"  bgcolor="#ffffd0">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='coqgym_image'>
                  <img src='images/coqgym-1.jpg' width="150">
                </div> 
                <img src='images/coqgym-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function coqgym_start() {
                  document.getElementById('coqgym_image').style.opacity = "1";
                }

                function coqgym_stop() {
                  document.getElementById('coqgym_image').style.opacity = "0";
                }
                coqgym_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1905.09381">
                <papertitle>Learning to Prove Theorems via Interacting with Proof Assistants</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong> and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>International Conference on Machine Learning (ICML)</em>, 2019
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1905.09381">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/CoqGym">code</a>
              /
              <a target="_blank" href="data/coq_icml_slides.pdf">slides</a>
              /
              <a target="_blank" href="data/coq_icml_poster.pdf">poster</a>
              <p></p>
              <p>We introduce CoqGym, one of the first and largest datasets for theorem proving in proof assistants, and ASTactic, a deep learning prover generating tactics as programs. </p>
            </td>
          </tr>

          <tr onmouseout="spatialsense_stop()" onmouseover="spatialsense_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='spatialsense_image'>
                  <img src='images/spatialsense-1.jpg' width="150">
                </div>
                <img src='images/spatialsense-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function spatialsense_start() {
                  document.getElementById('spatialsense_image').style.opacity = "1";
                }

                function spatialsense_stop() {
                  document.getElementById('spatialsense_image').style.opacity = "0";
                }
                spatialsense_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1908.02660">
                <papertitle>SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a>, and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>International Conference on Computer Vision (ICCV)</em>, 2019
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1908.02660">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/SpatialSense">code</a>
              /
              <a target="_blank" href="data/spatialsense_iccv_poster.pdf">poster</a>
              <p></p>
              <p>We propose <em>Adversarial Crowdsourcing</em> to reduce dataset bias and use it to construct SpatialSense, a challenging dataset for recognizing spatial relations in images.</p>
            </td>
          </tr>

          <tr onmouseout="hourglass_stop()" onmouseover="hourglass_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hourglass_image'>
                  <img src='images/hourglass-1.jpg' width="150">
                </div>
                <img src='images/hourglass-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function hourglass_start() {
                  document.getElementById('hourglass_image').style.opacity = "1";
                }

                function hourglass_stop() {
                  document.getElementById('hourglass_image').style.opacity = "0";
                }
                hourglass_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1603.06937">
                <papertitle>Stacked Hourglass Networks for Human Pose Estimation</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://www.alejandronewell.com/">Alejandro Newell</a>,
              <strong>Kaiyu Yang</strong>, and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>European Conference on Computer Vision (ECCV)</em>, 2016
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1603.06937">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/pose-hg-train">code</a>
              <p></p>
              <p>We introduce <em>Stacked Hourglass Networks</em>—one of the most popular architectures for human pose estimation, object detection, and more.</p>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Workshops & Tutorials</heading>
		<p>I'm a co-organizer of the following events:</p>
		<ul>
		  <li><a target="_blank" href="https://mathai2023.github.io/">The 3rd Workshop on Mathematical Reasoning and AI @ NeurIPS 2023</a></li>
		  <li><a target="_blank" href="https://machine-learning-for-theorem-proving.github.io/">Tutorial on Machine Learning for Theorem Proving @ NeurIPS 2023</a></li>
		</ul>
            </td>
          </tr>
        </tbody></table>
	      
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Media</heading>
	      <p>My works are covered by:</p>
		<ul>
		  <li><a target="_blank" href="https://www.marktechpost.com/2023/07/01/can-llms-generate-mathematical-proofs-that-can-be-rigorously-checked-meet-leandojo-an-open-source-ai-playground-with-toolkits-benchmarks-and-models-for-large-language-models-to-prove-formal-theore/">Can LLMs Generate Mathematical Proofs that can be Rigorously Checked?</a>, MarkTechPost, 2023</li>
		  <li><a target="_blank" href="https://insights.princeton.edu/2022/01/tradeoff-privacy-algorithm-performance/">Exploring the Tradeoff Between Privacy and Algorithm Performance</a>, Princeton Insights, 2022</li>
		  <li><a target="_blank" href="https://engineering.princeton.edu/news/2020/02/12/researchers-devise-approach-reduce-biases-computer-vision-data-sets">Researchers Devise Approach to Reduce Biases in Computer Vision Data Sets</a>, Princeton Engineering News, 2020</li>
		  <li><a target="_blank" href="https://www.wired.com/story/ai-biased-how-scientists-trying-fix">AI Is Biased. Here's How Scientists Are Trying to Fix It</a>, Wired, 2019</li>
		</ul>
            </td>
          </tr>
        </tbody></table>

	      
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
		<ul>
		  <li>Teaching assistant, <a target="_blank" href="https://princeton-nlp.github.io/cos484/">COS 485/585: Natural Language Processing</a>, Princeton University</li>
		  <li>Head teaching assistant, <a target="_blank" href="https://dsa.cs.tsinghua.edu.cn/~deng/ds/index.htm">Data Structures and Algorithms</a>, Tsinghua University</li>
		</ul>
            </td>
          </tr>
        </tbody></table>

	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br />
              <p style="text-align:right;font-size:small;">
                Website template credit: <a target="_blank" href="https://jonbarron.info/" style="text-align:right;font-size:small;">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

</body>

</html>
