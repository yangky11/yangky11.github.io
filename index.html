<!DOCTYPE HTML>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0B5LHBSZYY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0B5LHBSZYY');
  </script>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kaiyu Yang</title>
  
  <meta name="author" content="Kaiyu Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
  <link rel="manifest" href="images/site.webmanifest">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1050px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:1.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kaiyu Yang</name>
              </p>
              <p>
                I am a research scientist at <a target="_blank" href="https://ai.meta.com/research/">Meta Fundamental AI Research (FAIR)</a>, New York. Before joining Meta, I was a postdoctoral scholar at Caltech, collaborating with <a style="text-decoration: none" target="_blank" href="http://www.yisongyue.com/">Yisong Yue</a> and <a style="text-decoration: none" target="_blank" href="https://www.cs.utexas.edu/~swarat/">Swarat Chaudhuri</a>.
                I received my Ph.D. from Princeton University, where I was advised by <a style="text-decoration: none" target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>. 
              </p>

              <p style="text-align:center">
                杨凯峪 &nbsp/&nbsp
                kaiyuy [at] alumni [dot] princeton [dot] edu &nbsp/&nbsp
                <a target="_blank" href="data/CV___Kaiyu_Yang.pdf">CV</a> &nbsp/&nbsp
                <!-- <a target="_blank" href="data/Bio.txt">Bio</a> &nbsp/&nbsp-->
                <a target="_blank"href="https://scholar.google.com/citations?hl=en&user=FciCu4EAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://github.com/yangky11">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Kaiyu Yang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Kaiyu Yang - Circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
	<tr>
            <td>
              <heading>News</heading>
		<ul style="list-style-type: none">
	      <li><img src="images/fire.png" height="25em" /><b>We released <a target="_blank" href="https://blog.goedel-prover.com/">Goedel-Prover-V2</a>, the strongest open-source theorem proving model to date.</b></li>
		  <li><img src="images/fire.png" height="25em" /><b>We introduced <a target="_blank" href="https://verina.io/">Verina</a>: a benchmark for verifiable code generation in Lean.</b></li>
		  <li><img src="images/fire.png" height="25em" /><b>We released a position paper&mdash;<a target="_blank" href="https://arxiv.org/abs/2412.16075">Formal Mathematical Reasoning: A New Frontier in AI</a></b></li>
		</ul>
            </td>
          </tr>
        </tbody></table>      
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
			  <p>I aim to build <strong>verifiable AI</strong> that produces results whose correctness can be trusted without labor-intensive human checking. Today’s AI can generate code, proofs, and arguments at scale, but verifying their correctness often takes more time than doing the work manually. I address this bottleneck by developing <strong><a href="#formal-mathematical-reasoning">AI capable of formal reasoning</a></strong>. Grounded in formal systems such as Lean, AI can generate solutions paired with concise specifications and machine-checkable proofs. Humans then verify only the specification, while the formal system guarantees correctness. This shifts the role of humans from tedious verification to high-level guidance, enabling faster and more reliable progress in AI-accelerated mathematics, software development, and scientific discovery.</p>  
              <p>My research spans AI for formal theorem proving (<a href="#leandojo">LeanDojo</a>, <a href="#coqgym">CoqGym</a>, <a href="#goedel2">Goedel-Prover</a>), autoformalization (<a href="#leaneuclid">LeanEuclid</a>), and applications in formal mathematics and verification, including solving competition-level problems (<a href="#lips">LIPS</a>), assisting mathematicians (<a href="#leancopilot">Lean Copilot</a>, <a href="#leanfinder">Lean Finder</a>), and generating verifiable code (<a href="#verina">Verina</a>). Together, these efforts point towards integrated AI systems that reason rigorously across informal and formal domains and produce verifiable solutions with guaranteed correctness.</p>
			</td>
          </tr>
		
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	  <tr onmouseout="proofoptimizer_stop()" onmouseover="proofoptimizer_start()" id="proofoptimizer">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='proofoptimizer_image'>
                  <img src='images/proofoptimizer-2.png' width="150">
                </div>
                <img src='images/proofoptimizer-1.png' width="150">
              </div>
              <script type="text/javascript">
                function proofoptimizer_start() {
                  document.getElementById('proofoptimizer_image').style.opacity = "1";
                }

                function proofoptimizer_stop() {
                  document.getElementById('proofoptimizer_image').style.opacity = "0";
                }
                lips_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://proof-optimizer.github.io/">
                <papertitle>ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations</papertitle>
	      </a>
              <br />
	      <a target="_blank" href="https://minimario.github.io/">Alex Gu</a>, <a target="_blank" href="https://www.bartosz.cc/">Bartosz Piotrowski</a>, <a target="_blank" href="https://scholar.google.com/citations?user=_dF0fX4AAAAJ">Fabian Gloeckle</a>, <strong>Kaiyu Yang</strong>, <a target="_blank" href="https://www.markosyanaram.com/">Aram Markosyan</a>
              <br />
              <em>International Conference on Learning Representations (ICLR)</em>, 2026
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2510.15700">arXiv</a>
	       /
              <a target="_blank" href="https://proof-optimizer.github.io/">project</a>
	       /
              <a target="_blank" href="https://proof-optimizer.github.io/demo.html">demo</a>
              <p></p>
              <p>LLMs trained with reinforcement learning often produce formal proofs that are correct but excessively long and opaque. We introduce ProofOptimizer, a system that automatically shortens and cleans up these proofs while preserving correctness. It reduces proof length by up to 87% on MiniF2F and 57% on PutnamBench, making formal proofs more readable and revealing clearer mathematical insights.</p>
            </td>
          </tr>

	  <tr id="leanfinder">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/leanfinder.jpg' width="150">
              </div>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://huggingface.co/spaces/delta-lab-ai/Lean-Finder">
                <papertitle>Lean Finder: Semantic Search for Mathlib That Understands User Intents</papertitle>
	      </a>
              <br />
	          <a target="_blank" href="https://mikeljl.github.io/">Jialin Lu</a>, <a target="_blank" href="https://scholar.google.com/citations?user=1Q1pIwwAAAAJ&hl=en">Kye Emond</a>, <strong>Kaiyu Yang</strong>, <a target="_blank" href="https://www.cs.utexas.edu/~swarat/">Swarat Chaudhuri</a>, <a target="_blank" href="https://weiran-sun.github.io/">Weiran Sun</a>, <a target="_blank" href="https://delta-lab-ai.github.io/">Wuyang Chen</a>
              <br />
              <em>International Conference on Learning Representations (ICLR)</em>, 2026
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2510.15940">arXiv</a>
	       /
              <a target="_blank" href="https://huggingface.co/spaces/delta-lab-ai/Lean-Finder">project</a>
              <p></p>
              <p>We present Lean Finder, a semantic search engine for Lean and mathlib that understands and aligns with the intents of mathematicians.</p>
            </td>
          </tr>
			
	  <tr id="goedel2">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/goedel2.png' width="150">
              </div>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://blog.goedel-prover.com/">
                <papertitle>Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction</papertitle>
	      </a>
              <br />
	      <a target="_blank" href="https://linyongver.github.io/Website/">Yong Lin</a>*, <a target="_blank" href="https://shangetang.github.io/">Shange Tang</a>*, <a target="_blank" href="https://lyubh.cn/">Bohan Lyu</a>*, <a target="_blank" href="https://ziranyang0.github.io/">Ziran Yang</a>*, <a target="_blank" href="https://web.math.princeton.edu/~jc1220/">Jui-Hui Chung</a>*, <a target="_blank" href="https://hyzhao.me/">Haoyu Zhao</a>*, <a target="_blank" href="https://jianglai-0023.github.io/">Lai Jiang</a>*, <a target="_blank" href="https://scholar.google.com/citations?user=9jts-VQAAAAJ&hl=zh-CN">Yihan Geng</a>*, <a target="_blank" href="https://jiaweige0416.github.io/">Jiawei Ge</a>, <a target="_blank" href="https://jingruo.github.io/">Jingruo Sun</a>, <a target="_blank" href="https://ic-hub.github.io/">Jiayun Wu</a>, <a target="_blank" href="https://jirigesi.github.io/">Jiri Gesi</a>, <a target="_blank" href="https://www.cs.toronto.edu/~davidj/">David Acuna</a>, <strong>Kaiyu Yang</strong>, <a target="_blank" href="http://www.hongzhoulin.com/">Hongzhou Lin</a>*, <a target="_blank" href="https://yejinc.github.io/">Yejin Choi</a>, <a target="_blank" href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>, <a target="_blank" href="https://www.cs.princeton.edu/~arora/">Sanjeev Arora</a>, <a target="_blank" href="https://sites.google.com/view/cjin/home">Chi Jin</a>*
              <br />
              <em>International Conference on Learning Representations (ICLR)</em>, 2026
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2508.03613">arXiv</a>
	       /
              <a target="_blank" href="https://blog.goedel-prover.com/">project</a>
	       /
              <a target="_blank" href="https://github.com/Goedel-LM/Goedel-Prover-V2">code</a>
              <p></p>
              <p>Our Goedel-Prover-V2 doubled the SOTA Pass@32 performance on PutnamBench with a 20x smaller model.</p>
            </td>
          </tr>
	      
	  <tr onmouseout="veria_stop()" onmouseover="verina_start()" id="verina">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/verina.png' width="150">
              </div>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://verina.io/">
                <papertitle>Verina: Benchmarking Verifiable Code Generation</papertitle>
	      </a>
              <br />
	      <a target="_blank" href="https://github.com/leaferx">Zhe Ye</a>, <a>Zhengxu Yan</a>, <a target="_blank" href="https://jxhe.info/">Jingxuan He</a>, <a>Timothe Kasriel</a>, <strong>Kaiyu Yang</strong>, <a target="_blank" href="https://dawnsong.io/">Dawn Song</a>
              <br />
              <em>International Conference on Learning Representations (ICLR)</em>, 2026
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2505.23135">arXiv</a>
		/
	      <a target="_blank" href="https://verina.io/">project</a>
		/
              <a target="_blank" href="https://huggingface.co/datasets/sunblaze-ucb/verina">data</a>
		/
              <a target="_blank" href="https://github.com/sunblaze-ucb/verina">code</a>
              <p></p>
              <p>We introduce Verina (<u>Veri</u>fiable Code Generation Are<u>na</u>), a high-quality benchmark for <em>verifiable</em> code generation (joint generation of code, specifications, and proofs) in Lean</p>
            </td>
          </tr>

	<!--
	  <tr onmouseout="shortest_path_stop()" onmouseover="shortest_path_start()" id="shortest_path">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/shortest_path.png' width="150">
              </div>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://arxiv.org/abs/2502.08794">
                <papertitle>Spectral Journey: How Transformers Predict the Shortest Path</papertitle>
	      </a>
              <br />
	      <a>Andrew Cohen</a>, <a target="_blank" href="https://sites.google.com/view/andreygromov">Andrey Gromov</a>, <strong>Kaiyu Yang</strong>, <a target="_blank" href="https://yuandong-tian.com/">Yuandong Tian</a>
              <br />
              <em>Preprint</em>, 2025
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2502.08794">arXiv</a>
              <p></p>
              <p>We show that Transformers learn to predict shortest paths on graphs through a mechanism highly correlated with spectral decomposition.</p>
            </td>
          </tr>
	  -->

	  <tr id="formal-mathematical-reasoning">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/formal-mathematical-reasoning.png' width="150">
              </div>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://arxiv.org/abs/2412.16075">
                <papertitle>Formal Mathematical Reasoning: A New Frontier in AI</papertitle>
	      </a>
              <br />
	      <strong>Kaiyu Yang</strong>, <a target="_blank" href="https://gpoesia.com/">Gabriel Poesia</a>, <a target="_blank" href="https://jxhe.info/">Jingxuan He</a>, <a target="_blank" href="https://wenda302.github.io/">Wenda Li</a>, <a target="_blank" href="https://ai.meta.com/people/786716476205590/kristin-e-lauter/">Kristin Lauter</a>, <a target="_blank" href="https://www.cs.utexas.edu/~swarat/">Swarat Chaudhuri</a>, <a target="_blank" href="https://dawnsong.io/">Dawn Song</a>
              <br />
              <em>International Conference on Machine Learning (ICML), Position Papers Track</em>, 2025, <font color="red"><strong>Spotlight</strong></font>
	      <br />
	      A separate version accepted to <em>Communications of the ACM</em>
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2412.16075">arXiv</a>
	       /
	      <a target="_blank" href="https://cacm.acm.org/research/formal-reasoning-meets-llms-toward-ai-for-mathematics-and-verification/">CACM</a>
              <p></p>
              <p>This position paper advocates for formal mathematical reasoning, i.e., mathematical reasoning grounded in formal systems such as proof assistants. It is complementary to the informal approach (training LLMs on mathematical texts) and is arguably indispensable for advancing AI4Math to the next level.</p>
            </td>
          </tr>
		
	  <tr onmouseout="lips_stop()" onmouseover="lips_start()" id="lips">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lips_image'>
                  <img src='images/lips-2.png' width="150">
                </div>
                <img src='images/lips-1.png' width="150">
              </div>
              <script type="text/javascript">
                function lips_start() {
                  document.getElementById('lips_image').style.opacity = "1";
                }

                function lips_stop() {
                  document.getElementById('lips_image').style.opacity = "0";
                }
                lips_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://arxiv.org/abs/2502.13834">
                <papertitle>Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning</papertitle>
	      </a>
              <br />
	      <a target="_blank" href="https://lizn-zn.github.io/">Zenan Li</a>*, <a target="_blank" href="https://www.zhaoyu-li.com/">Zhaoyu Li</a>* <a>Wen Tang</a>, <a>Xian Zhang</a>, <a>Yuan Yao</a>, <a target="_blank" href="https://www.cs.toronto.edu/~six/">Xujie Si</a>, <a>Fan Yang</a>, <strong>Kaiyu Yang</strong>&dagger;, <a target="_blank" href="https://ics.nju.edu.cn/people/xiaoxingma/">Xiaoxing Ma</a>&dagger; (&dagger; equal advising)
              <br />
              <em>International Conference on Learning Representations (ICLR)</em>, 2025
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2502.13834">arXiv</a>
	       /
              <a target="_blank" href="https://github.com/Lizn-zn/NeqLIPS">code</a>
              <p></p>
              <p>To prove inequalities in math competitions, we analyze and distill human techniques into scaling and rewriting, which are well-suited for symbolic methods and LLMs respectively. By integrating LLMs with domain-specific mathematical insights, our approach substantially outperforms existing methods.</p>
            </td>
          </tr>

	  <tr onmouseout="goedel_stop()" onmouseover="goedel_start()" id="goedel">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='goedel_image'>
                  <img src='images/goedel-2.png' width="150">
                </div>
                <img src='images/goedel-1.png' width="150">
              </div>
              <script type="text/javascript">
                function goedel_start() {
                  document.getElementById('goedel_image').style.opacity = "1";
                }

                function goedel_stop() {
                  document.getElementById('goedel_image').style.opacity = "0";
                }
                goedel_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://goedel-lm.github.io/">
                <papertitle>Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving</papertitle>
	      </a>
              <br />
	      <a target="_blank" href="https://linyongver.github.io/Website/">Yong Lin</a>*, <a target="_blank" href="https://shangetang.github.io/">Shange Tang</a>*, <a target="_blank" href="https://lyubh.cn/">Bohan Lyu</a>, <a target="_blank" href="https://ic-hub.github.io/">Jiayun Wu</a>, <a target="_blank" href="http://www.hongzhoulin.com/">Hongzhou Lin</a>, <strong>Kaiyu Yang</strong>, <a>Jia Li</a>, <a target="_blank" href="https://xiamengzhou.github.io/">Mengzhou Xia</a>, <a target="_blank" href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>, <a target="_blank" href="https://www.cs.princeton.edu/~arora/">Sanjeev Arora</a>, <a target="_blank" href="https://sites.google.com/view/cjin/home">Chi Jin</a>
              <br />
              <em>Conference on Language Modeling (COLM)</em>, 2025
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2502.07640">arXiv</a>
	       /
              <a target="_blank" href="https://goedel-lm.github.io/">project</a>
	       /
              <a target="_blank" href="https://github.com/Goedel-LM/Goedel-Prover">code</a>
              <p></p>
              <p>We introduce Goedel-Prover, an open-source, state-of-the-art LLM for automated theorem proving in Lean. The key is to synthesize 1.64 million formal statements through autoformalization.</p>
            </td>
          </tr>
		
	  <tr onmouseout="lean_copilot_stop()" onmouseover="lean_copilot_start()" id="leancopilot">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lean_copilot_image'>
                  <img src='images/lean_copilot-1.png' width="150">
                </div>
                <img src='images/lean_copilot-2.png' width="150">
              </div>
              <script type="text/javascript">
                function lean_copilot_start() {
                  document.getElementById('lean_copilot_image').style.opacity = "1";
                }

                function lean_copilot_stop() {
                  document.getElementById('lean_copilot_image').style.opacity = "0";
                }
                lean_copilot_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://arxiv.org/abs/2404.12534">
                  <papertitle>Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean</papertitle>
	      </a>
              <br />
	      <a target="_blank" href="https://peiyang-song.github.io/">Peiyang Song</a>, <strong>Kaiyu Yang</strong>, <a target="_blank", href="http://tensorlab.cms.caltech.edu/users/anima/index.html">Anima Anandkumar</a>
              <br />
              <em>International Conference on Neuro-symbolic Systems (NeuS)</em>, 2025
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2404.12534">arXiv</a>
	       /
              <a target="_blank" href="https://github.com/lean-dojo/LeanCopilot">code</a>
	       /
	      <a target="_blank" href="https://youtu.be/OxFcZU5ihBk">demo</a>
	       /
	      <a target="_blank" href="https://www.youtube.com/watch?v=7NAIXBANSj4">talk</a>
	       /
	      <a target="_blank" href="https://www.scientificamerican.com/article/mathematicians-newest-assistants-are-artificially-intelligent/">media</a>
              <p></p>
              <p>
                We introduce a framework for running neural network inference directly in Lean. It enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users, including tools for suggesting proof steps and completing intermediate proof goals using LLMs.
              </p>
            </td>
          </tr>
		
	  <tr id="sciglm">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/sciglm-1.png' width="150">
              </div>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://arxiv.org/abs/2401.07950">
		<papertitle>SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models</papertitle>
	      </a>
              <br />
	      <a target="_blank" href="https://zhangdan0602.github.io/">Dan Zhang</a>, <a target="_blank" href="https://acbull.github.io/">Ziniu Hu</a>, <a>Sining Zhoubian</a>, <a target="_blank" href="https://zxdu.xyz/">Zhengxiao Du</a>, <strong>Kaiyu Yang</strong>, <a>Zihan Wang</a>, <a target="_blank" href="http://www.yisongyue.com/">Yisong Yue</a>, <a target="_blank" href="https://keg.cs.tsinghua.edu.cn/yuxiao/">Yuxiao Dong</a>, <a target="_blank" href="https://keg.cs.tsinghua.edu.cn/jietang/">Jie Tang</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track</em>, 2024
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2401.07950">arXiv</a>
	       /
	      <a target="_blank" href="https://github.com/THUDM/SciGLM">code</a>
              <p></p>
              <p>
                We curated <em>SciInstruct</em>, a diverse and high-quality dataset of college-level mathematics, physics, chemistry, and formal proofs. Using SciInstruct to finetune the ChatGLM family of LLMs, we introduce <em>SciGLM</em>, a suite of scientific language models for college-level mathematical/scientific reasoning. 
              </p>
            </td>
          </tr>
		
	  <tr id="ntp_survey">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/ntp_survey.jpg' width="150">
              </div>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://arxiv.org/abs/2404.09939">
                <papertitle>A Survey on Deep Learning for Theorem Proving</papertitle>
	      </a>
              <br />
	      <a target="_blank" href="https://www.zhaoyu-li.com/">Zhaoyu Li</a>, <a target="_blank" href="https://www.linkedin.com/in/jack-sun-2741711b5/?originalSubdomain=ca">Jialiang Sun</a>, <a target="_blank" href="https://www.cs.toronto.edu/~lmurphy/">Logan Murphy</a>, <a target="_blank" href="https://www.cs.toronto.edu/~qdsu/">Qidong Su</a>, <a target="_blank" href="https://scholar.google.com/citations?user=eu4eqTcAAAAJ&hl=zh-CN">Zenan Li</a>, <a target="_blank" href="https://www.microsoft.com/en-us/research/people/zhxian/">Xian Zhang</a>, <strong>Kaiyu Yang</strong>, <a target="_blank" href="https://www.cs.toronto.edu/~six/">Xujie Si</a>
              <br />
              <em>Conference on Language Modeling (COLM)</em>, 2024
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2404.09939">arXiv</a>
	       /
	      <a target="_blank" href="https://github.com/zhaoyu-li/DL4TP">code</a>
              <p></p>
              <p>
        	We present the first comprehensive survey of deep learning for theorem proving and autoformalization.
              </p>
            </td>
          </tr>
		
	  <tr onmouseout="lean_euclid_stop()" onmouseover="lean_euclid_start()" id="leaneuclid">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lean_euclid_image'>
                  <img src='images/lean_euclid-1.png' width="150">
                </div>
                <img src='images/lean_euclid-2.png' width="150">
              </div>
              <script type="text/javascript">
                function lean_euclid_start() {
                  document.getElementById('lean_euclid_image').style.opacity = "1";
                }

                function lean_euclid_stop() {
                  document.getElementById('lean_euclid_image').style.opacity = "0";
                }
                lean_euclid_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://arxiv.org/abs/2405.17216">
                <papertitle>Autoformalizing Euclidean Geometry</papertitle>
	      </a>
              <br />
	      <a target="_blank" href="https://www.cs.toronto.edu/~lmurphy/">Logan Murphy</a>*,  <strong>Kaiyu Yang</strong>*, <a target="_blank" href="https://www.linkedin.com/in/jack-sun-2741711b5/?originalSubdomain=ca">Jialiang Sun</a>, <a target="_blank" href="https://www.zhaoyu-li.com/">Zhaoyu Li</a>, <a target="_blank", href="http://tensorlab.cms.caltech.edu/users/anima/index.html">Anima Anandkumar</a>, <a target="_blank" href="https://www.cs.toronto.edu/~six/">Xujie Si</a> (* equal contribution)
              <br />
              <em>International Conference on Machine Learning (ICML)</em>, 2024
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2405.17216">arXiv</a>
	       /
              <a target="_blank" href="https://github.com/loganrjmurphy/LeanEuclid">code</a>
              <p></p>
              <p>
        	We release <em>LeanEuclid</em>, a benchmark for testing autoformalization, consisting of Euclid's <em>Elements</em> (Book I) manually formalized in Lean. It is challenging for state-of-the-art LLMs like GPT-4V. Furthermore, the process of constructing LeanEuclid has uncovered intriguing ambiguities in Euclid's original works.
              </p>
            </td>
          </tr>
	
          <tr onmouseout="leandojo_stop()" onmouseover="leandojo_start()" id="leandojo">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='leandojo_image'>
                  <img src='images/leandojo-1.jpg' width="150">
                </div>
                <img src='images/leandojo-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function leandojo_start() {
                  document.getElementById('leandojo_image').style.opacity = "1";
                }

                function leandojo_stop() {
                  document.getElementById('leandojo_image').style.opacity = "0";
                }
                leandojo_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://leandojo.org/leandojo">
                <papertitle>LeanDojo: Theorem Proving with Retrieval-Augmented Language Models</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, <a target="_blank" href="https://aidanswope.com/about">Aidan Swope</a>, <a target="_blank" href="https://minimario.github.io/">Alex Gu</a>, <a target="_blank" href="https://rchalamala.github.io/">Rahul Chalamala</a>, <a target="_blank" href="https://peiyang-song.github.io/">Peiyang Song</a>, <a target="_blank" href="https://billysx.github.io/">Shixing Yu</a>, <a target="_blank" href="https://www.linkedin.com/in/saad-godil-9728353/">Saad Godil</a>, <a target="_blank" href="https://www.linkedin.com/in/ryan-prenger-18797ba1/">Ryan Prenger</a>, <a target="_blank", href="http://tensorlab.cms.caltech.edu/users/anima/index.html">Anima Anandkumar</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track</em>, 2023, <font color="red"><strong>Oral presentation</strong></font>
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2306.15626">arXiv</a>
	      /
              <a target="_blank" href="https://leandojo.org/leandojo">project</a>
	      /
	      <a target="_blank" href="https://github.com/orgs/lean-dojo">code</a>
	      /
	      <a target="_blank" href="https://www.youtube.com/watch?v=qvaR_VXB4fA&feature=youtu.be">talk</a>
	      /
	      <a target="_blank" href="data/LeanDojo_slides.pdf">slides</a>
	      /
	      <a target="_blank" href="https://www.marktechpost.com/2023/07/01/can-llms-generate-mathematical-proofs-that-can-be-rigorously-checked-meet-leandojo-an-open-source-ai-playground-with-toolkits-benchmarks-and-models-for-large-language-models-to-prove-formal-theore/">media</a>
              <p></p>
              <p>Can LLMs generate mathematical proofs that can be rigorously checked? We release <em>LeanDojo</em>: an open-source playground consisting of toolkits, benchmarks, and models for LLMs to prove formal theorems in the Lean proof assistant.</p>
            </td>
          </tr>	
		
          <tr>
            <td style="padding:5px;width:17%;vertical-align:middle">
		<video width="150" controls>
		  <source src="images/infinigen.mp4" type="video/mp4">
		  Your browser does not support HTML video.
		</video>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://infinigen.org/">
                <papertitle>Infinite Photorealistic Worlds using Procedural Generation</papertitle>
              </a>
              <br />
	      <a target="_blank" href="https://araistrick.github.io/">Alexander Raistrick</a>*, <a target="_blank" href=https://www.lahavlipson.com/"">Lahav Lipson</a>*, <a target="_blank" href="https://mazeyu.github.io/">Zeyu Ma</a>*, <a target="_blank" href="https://www.cs.princeton.edu/~lm5483/">Lingjie Mei</a>, <a target="_blank" href="https://www.cs.princeton.edu/~mingzhew/">Mingzhe Wang</a>, <a target="_blank" href="https://zuoym15.github.io/">Yiming Zuo</a>, <a target="_blank" href="https://kkayan.com/">Karhan Kayan</a>, <a target="_blank" href="https://hermera.github.io/">Hongyu Wen</a>, <a target="_blank" href="">Beining Han</a>, <a target="_blank" href="">Yihan Wang</a>, <a target="_blank" href="https://www.alejandronewell.com/">Alejandro Newell</a>, <a target="_blank" href="https://heilaw.github.io/">Hei Law</a>, <a target="_blank" href="https://imankgoyal.github.io/">Ankit Goyal</a>, <strong>Kaiyu Yang</strong>, <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2306.09310">arXiv</a>
	      /
	      <a target="_blank" href="https://infinigen.org/">project</a>
	      /
              <a target="_blank" href="https://github.com/princeton-vl/infinigen">code</a>
              <p></p>
              <p>
                Data drives progress in computer vision. We introduce <em>Infinigen</em>: a generator of unlimited high-quality 3D data. 100% procedural, no external assets, no AI. Free and open source.
              </p>
            </td>
          </tr>	
		  
          <tr onmouseout="metaqnl_stop()" onmouseover="metaqnl_start()" id="metaqnl">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='metaqnl_image'>
                  <img src='images/metaqnl-1.jpg' width="150">
                </div>
                <img src='images/metaqnl-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function metaqnl_start() {
                  document.getElementById('metaqnl_image').style.opacity = "1";
                }

                function metaqnl_stop() {
                  document.getElementById('metaqnl_image').style.opacity = "0";
                }
                metaqnl_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2111.12038">
                <papertitle>Learning Symbolic Rules for Reasoning in Quasi-Natural Language</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong> and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Transactions on Machine Learning Research (TMLR)</em>, 2023
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2111.12038">arXiv</a>
	      /
              <a target="_blank" href="https://github.com/princeton-vl/MetaQNL">code</a>
              <p></p>
              <p>
                We propose <em>MetaQNL</em>, a symbolic "Quasi-Natural" language that can express both formal logic and natural language. Instead of manually constructing MetaQNL rules, we propose <em>MetaInduce</em>: an algorithm for learning rules from data. 
              </p>
            </td>
          </tr>		
          
          
          <tr onmouseout="nlproofs_stop()" onmouseover="nlproofs_start()" id="nlproofs">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nlproofs_image'>
                  <img src='images/nlproofs-1.jpg' width="150">
                </div>
                <img src='images/nlproofs-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function nlproofs_start() {
                  document.getElementById('nlproofs_image').style.opacity = "1";
                }

                function nlproofs_stop() {
                  document.getElementById('nlproofs_image').style.opacity = "0";
                }
                nlproofs_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2205.12443">
                <papertitle>Generating Natural Language Proofs with Verifier-Guided Search</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, <a target="_blank" href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>
              <br />
              <em>Empirical Methods in Natural Language Processing (EMNLP)</em>, 2022, <font color="red"><strong>Oral presentation</strong></font>
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2205.12443">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-nlp/NLProofS">code</a>
              /
              <a target="_blank" href="data/nlproofs_slides.pdf">slides</a>
              <p></p>
              <p>
                We introduce <em>NLProofS</em> (<u>N</u>atural <u>L</u>anguage <u>Proof</u> <u>S</u>earch) for multi-step logical reasoning in natural language. Given a hypothesis and a set of supporting facts, it generates a proof tree indicating how to derive the hypothesis from supporting facts.
              </p>
            </td>
          </tr>

          <tr onmouseout="imagenet_privacy_stop()" onmouseover="imagenet_privacy_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='imagenet_privacy_image'>
                  <img src='images/imagenet_privacy-1.jpg' width="150">
                </div>
                <img src='images/imagenet_privacy-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function imagenet_privacy_start() {
                  document.getElementById('imagenet_privacy_image').style.opacity = "1";
                }

                function imagenet_privacy_stop() {
                  document.getElementById('imagenet_privacy_image').style.opacity = "0";
                }
                imagenet_privacy_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="http://image-net.org/face-obfuscation">
                <papertitle>A Study of Face Obfuscation in ImageNet</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.linkedin.com/in/jacqueline-yau-836b0a132">Jacqueline Yau</a>, 
              <a target="_blank" href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, <a target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a> 
              <br />
              <em>International Conference on Machine Learning (ICML)</em>, 2022
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2103.06191">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princetonvisualai/imagenet-face-obfuscation">code</a>
              /
              <a target="_blank" href="data/imagenet_privacy_slides.pdf">slides</a>
              /
              <a target="_blank" href="https://youtu.be/VnszYe8abRs?t=889">talk</a>
              /
              <a target="_blank" href="http://image-net.org/face-obfuscation">project</a>
              /
              <a target="_blank" href="https://insights.princeton.edu/2022/01/tradeoff-privacy-algorithm-performance">media</a>
              <p></p>
              <p>We annotate human faces in ImageNet and obfuscate them for privacy protection. We show that face obfuscation does not hurt image classification and transfer learning.</p>
            </td>
          </tr>

          <tr onmouseout="attach_juxtapose_stop()" onmouseover="attach_juxtapose_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='attach_juxtapose_image'>
                  <img src='images/attach_juxtapose-1.jpg' width="150">
                </div>
                <img src='images/attach_juxtapose-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function attach_juxtapose_start() {
                  document.getElementById('attach_juxtapose_image').style.opacity = "1";
                }

                function attach_juxtapose_stop() {
                  document.getElementById('attach_juxtapose_image').style.opacity = "0";
                }
                attach_juxtapose_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2010.14568">
                <papertitle>Strongly Incremental Constituency Parsing with Graph Neural Networks</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong> and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2020
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2010.14568">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/attach-juxtapose-parser">code</a>
              /
              <a target="_blank" href="data/parsing_neurips_slides.pdf">slides</a>
              /
              <a target="_blank" href="https://crossminds.ai/video/strongly-incremental-constituency-parsing-with-graph-neural-networks-606fee03f43a7f2f827c13c3/">talk</a>
              <p></p>
              <p>We propose a novel transition-based constituency parser named <em>Attach-Juxtapose</em>, inspired by how humans perform parsing.</p>
            </td>
          </tr>
					
          <tr>
            <td style="padding:5px;width:17%;vertical-align:middle">
              <img src='images/rel3d.gif' width="150">
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2012.01634">
                <papertitle>Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://imankgoyal.github.io/">Ankit Goyal</a>, 
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="http://www-personal.umich.edu/~ydawei/">Dawei Yang</a>, <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2020, <font color="red"><strong>Spotlight</strong></font>
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2012.01634">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/Rel3D">code</a>
              <p></p>
              <p>We propose <em>Minimally Contrastive Data Collection</em>: a novel crowdsourcing method for reducing dataset bias. And we use it to construct Rel3D—the first large-scale, human-annotated dataset for grounding spatial relations in 3D.</p>
            </td>
          </tr>

          <tr onmouseout="imagenet_fairness_stop()" onmouseover="imagenet_fairness_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='imagenet_fairness_image'>
                  <img src='images/imagenet_fairness-1.jpg' width="150">
                </div>  
                <img src='images/imagenet_fairness-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function imagenet_fairness_start() {
                  document.getElementById('imagenet_fairness_image').style.opacity = "1";
                }

                function imagenet_fairness_stop() {
                  document.getElementById('imagenet_fairness_image').style.opacity = "0";
                }
                imagenet_fairness_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1912.07726">
                <papertitle>Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.linkedin.com/in/klint-qinami/">Klint Qinami</a>, 
              <a target="_blank" href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, <a target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a>
              <br />
              <em>Conference on Fairness, Accountability, and Transparency (FAT*)</em>, 2020
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1912.07726">arXiv</a>
              /
              <a target="_blank" href="data/imagenet_debias_fat_slides.pdf">slides</a>
              /
              <a target="_blank" href="https://www.youtube.com/watch?v=MLaqxuNtgAo">talk</a>
              /
              <a target="_blank" href="http://image-net.org/update-sep-17-2019">blog</a>
              /
              <a target="_blank" href="https://www.wired.com/story/ai-biased-how-scientists-trying-fix">media</a>
              <p></p>
              <p>We reveal and mitigate fairness issues of ImageNet, filtering its concept vocabulary and balancing its representation of various demographic groups in images.</p>
            </td>
          </tr>

          <tr onmouseout="coqgym_stop()" onmouseover="coqgym_start()"  id="coqgym">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='coqgym_image'>
                  <img src='images/coqgym-1.jpg' width="150">
                </div> 
                <img src='images/coqgym-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function coqgym_start() {
                  document.getElementById('coqgym_image').style.opacity = "1";
                }

                function coqgym_stop() {
                  document.getElementById('coqgym_image').style.opacity = "0";
                }
                coqgym_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1905.09381">
                <papertitle>Learning to Prove Theorems via Interacting with Proof Assistants</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong> and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>International Conference on Machine Learning (ICML)</em>, 2019
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1905.09381">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/CoqGym">code</a>
              /
              <a target="_blank" href="data/coq_icml_slides.pdf">slides</a>
              <p></p>
              <p>We introduce CoqGym, one of the first and largest datasets for theorem proving in proof assistants, and ASTactic, a deep learning prover generating tactics as programs. </p>
            </td>
          </tr>

          <tr onmouseout="spatialsense_stop()" onmouseover="spatialsense_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='spatialsense_image'>
                  <img src='images/spatialsense-1.jpg' width="150">
                </div>
                <img src='images/spatialsense-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function spatialsense_start() {
                  document.getElementById('spatialsense_image').style.opacity = "1";
                }

                function spatialsense_stop() {
                  document.getElementById('spatialsense_image').style.opacity = "0";
                }
                spatialsense_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1908.02660">
                <papertitle>SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a>, <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>International Conference on Computer Vision (ICCV)</em>, 2019
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1908.02660">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/SpatialSense">code</a>
              <p></p>
              <p>We propose <em>Adversarial Crowdsourcing</em> to reduce dataset bias and use it to construct SpatialSense, a challenging dataset for recognizing spatial relations in images.</p>
            </td>
          </tr>

          <tr onmouseout="hourglass_stop()" onmouseover="hourglass_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hourglass_image'>
                  <img src='images/hourglass-1.jpg' width="150">
                </div>
                <img src='images/hourglass-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function hourglass_start() {
                  document.getElementById('hourglass_image').style.opacity = "1";
                }

                function hourglass_stop() {
                  document.getElementById('hourglass_image').style.opacity = "0";
                }
                hourglass_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1603.06937">
                <papertitle>Stacked Hourglass Networks for Human Pose Estimation</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://www.alejandronewell.com/">Alejandro Newell</a>,
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>European Conference on Computer Vision (ECCV)</em>, 2016
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1603.06937">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/pose-hg-train">code</a>
              <p></p>
              <p>We introduce <em>Stacked Hourglass Networks</em>—one of the most popular architectures for human pose estimation, object detection, and more.</p>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Open Source Contributions</heading>
		<p>I'm a core developer of the following GitHub repos:</p>
		<ul>
		  <li><a target="_blank" href="https://github.com/lean-dojo/LeanDojo">LeanDojo <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lean-dojo/LeanDojo"></li>
		  <li><a target="_blank" href="https://github.com/lean-dojo/ReProver">ReProver <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lean-dojo/ReProver"></li>
		  <li><a target="_blank" href="https://github.com/lean-dojo/LeanCopilot">LeanCopilot <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/lean-dojo/LeanCopilot"></li>
		  <li><a target="_blank" href="https://github.com/princeton-vl/CoqGym">CoqGym <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/princeton-vl/CoqGym"></li>
		</ul>
            </td>
          </tr>
        </tbody></table>
			
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Workshops & Tutorials</heading>
		<p>I'm a co-organizer of the following events:</p>
		<ul>
		  <li><a target="_blank" href="https://mathai2025.github.io/">The 5th Workshop on Mathematical Reasoning and AI</a> @ NeurIPS 2025</li>
		  <li><a target="_blank" href="https://mathai2023.github.io/">The 3rd Workshop on Mathematical Reasoning and AI</a> @ NeurIPS 2023 (<a target="_blank" href="https://neurips.cc/virtual/2023/workshop/66522">Video</a>)</li>
		  <li><a target="_blank" href="https://machine-learning-for-theorem-proving.github.io/">Tutorial on Machine Learning for Theorem Proving</a> @ NeurIPS 2023 (<a target="_blank" href="https://neurips.cc/virtual/2023/tutorial/73946">Video</a>)</li>
		</ul>
            </td>
          </tr>
        </tbody></table>
	      
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Media</heading>
	      <p>My works are covered by:</p>
		<ul>
		  <li><a target="_blank" href="https://www.scientificamerican.com/article/mathematicians-newest-assistants-are-artificially-intelligent/">Mathematicians’ Newest Assistants Are Artificially Intelligent</a>, Scientific American, 2024</li>
		  <li><a target="_blank" href="https://www.marktechpost.com/2023/07/01/can-llms-generate-mathematical-proofs-that-can-be-rigorously-checked-meet-leandojo-an-open-source-ai-playground-with-toolkits-benchmarks-and-models-for-large-language-models-to-prove-formal-theore/">Can LLMs Generate Mathematical Proofs that can be Rigorously Checked?</a>, MarkTechPost, 2023</li>
		  <li><a target="_blank" href="https://insights.princeton.edu/2022/01/tradeoff-privacy-algorithm-performance/">Exploring the Tradeoff Between Privacy and Algorithm Performance</a>, Princeton Insights, 2022</li>
		  <li><a target="_blank" href="https://engineering.princeton.edu/news/2020/02/12/researchers-devise-approach-reduce-biases-computer-vision-data-sets">Researchers Devise Approach to Reduce Biases in Computer Vision Data Sets</a>, Princeton Engineering News, 2020</li>
		  <li><a target="_blank" href="https://www.wired.com/story/ai-biased-how-scientists-trying-fix">AI Is Biased. Here's How Scientists Are Trying to Fix It</a>, Wired, 2019</li>
		</ul>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading id="mentoring">Mentoring</heading>
		<p>I'm fortunate to have worked with many talented students and junior researchers:</p>
		<ul>
		  <li><a target="_blank" href="https://www.zhaoyu-li.com/">Zhaoyu Li</a>: Ph.D. student @ University of Toronto</li>
		  <li><a target="_blank" href="https://jc-chen1.github.io/">Jiacheng Chen</a>: Undergrad @ South China University of Technology -> Ph.D. student @ Chinese University of Hong Kong</li>
		  <li><a target="_blank" href="https://peiyang-song.github.io/">Peiyang Song</a>: Undergrad @ UCSB -> Undergrad @ Caltech</li>
		  <li><a target="_blank" href="https://chalamala.com/">Rahul Chalamala</a>: Undergrad @ Caltech -> Researcher @ Together AI</li>
		  <li><a target="_blank" href="https://billysx.github.io/">Shixing Yu</a>: Master's student @ UT Austin -> Ph.D. student @ Cornell</li>
		  <li><a target="_blank" href="https://genechou.com/">Gene Chou</a>: Undergrad @ Princeton -> Ph.D. student @ Cornell </li>
		  <li><a target="_blank" href="https://www.linkedin.com/in/jacqueline-yau-836b0a132/">Jacqueline Yau</a>: Master’s student @ Stanford -> Ph.D. student @ UIUC </li>
		</ul>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
		<ul>
		  <li>Guest Co-instructor, <a target="_blank" href="https://llmagents-learning.org/sp25">Advanced Large Language Model Agents</a>, UC Berkeley &amp; MOOC</li>
		  <li>Guest Lecturer, <a target="_blank" href="https://awesome-genai.github.io/">AIST 5030: Generative Artificial Intelligence</a>, Chinese University of Hong Kong</li>
		  <li>Guest Lecturer, <a target="_blank" href="https://sites.google.com/view/cs-159-2024/home">CS 159: Large Language Models for Reasoning</a>, Caltech</li>
		  <li>Teaching Assistant, <a target="_blank" href="https://nlp.cs.princeton.edu/cos484-sp21/">COS 485/585: Natural Language Processing</a>, Princeton University</li>
		  <li>Head Teaching Assistant, <a target="_blank" href="https://dsa.cs.tsinghua.edu.cn/~deng/ds/index.htm">Data Structures and Algorithms</a>, Tsinghua University</li>
		</ul>
            </td>
          </tr>
        </tbody></table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
		<ul>
		  <li><strong>Area Chair</strong>: ECCV 2024, ICML 2025, ICML 2026</li>
		  <li><strong>Reviewer</strong>: ICML, NeurIPS, ICLR, etc.</li>
		</ul>
            </td>
          </tr>
        </tbody></table>
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br />
              <p style="text-align:right;font-size:small;">
                Website template credit: <a target="_blank" href="https://jonbarron.info/" style="text-align:right;font-size:small;">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

</body>

</html>
