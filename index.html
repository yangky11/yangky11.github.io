<!DOCTYPE HTML>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0B5LHBSZYY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0B5LHBSZYY');
  </script>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kaiyu Yang</title>
  
  <meta name="author" content="Kaiyu Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
  <link rel="manifest" href="images/site.webmanifest">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:1.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kaiyu Yang</name>
              </p>
              <p>
                I am a research scientist at Meta in the <a target="_blank" href="https://ai.meta.com/research/">Fundamental AI Research (FAIR)</a> team. I am based in New York and work on AI/LLMs for mathematical reasoning. Before joining Meta, I was a postdoctoral scholar at Caltech, working with <a style="text-decoration: none" target="_blank" href="https://www.vision.caltech.edu/">Pietro Perona</a>, <a style="text-decoration: none" target="_blank" href="http://www.yisongyue.com/">Yisong Yue</a>, and <a style="text-decoration: none" target="_blank" href="https://www.cs.utexas.edu/~swarat/">Swarat Chaudhuri</a>.
                I received my Ph.D. from Princeton University, where I was advised by <a style="text-decoration: none" target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a> 
                and also worked with <a style="text-decoration: none" target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a> and <a style="text-decoration: none" target="_blank" href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>. 
              </p>

              <p style="text-align:center">
                杨凯峪 &nbsp/&nbsp
                kaiyuy [MASK] meta [MASK] com &nbsp/&nbsp
                <a target="_blank" href="data/CV___Kaiyu_Yang.pdf">CV</a> &nbsp/&nbsp
                <a target="_blank" href="data/Bio.txt">Bio</a> &nbsp/&nbsp
                <a target="_blank"href="https://scholar.google.com/citations?hl=en&user=FciCu4EAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://github.com/yangky11">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Kaiyu Yang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Kaiyu Yang - Circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>News</heading>
		<ul style="list-style-type: none">
		  <li><img src="images/fire.png" height="25em" /><b>We released a position paper&mdash;<a target="_blank" href="https://arxiv.org/abs/2412.16075">Formal Mathematical Reasoning: A New Frontier in AI</a></b></li>
		  <li><img src="images/fire.png" height="25em" /><b style='color:red;'>My collaborator, <a target="_blank" href="https://jc-chen1.github.io/">Jiacheng Chen</a>, is currently applying for Ph.D. programs. Recruit him if you're looking for a strong student in AI for mathematical reasoning and theorem proving! Feel free to contact me to discuss his qualifications.</b></li>
		  <li><img src="images/fire.png" height="25em" /><b>We're looking for a postdoc to work on AI for formal mathematical reasoning. If you're interested, apply as soon as possible via <a href="https://www.metacareers.com/jobs/1459691901359421" target="_blank">this link</a> and email me. We will close the search very soon.</b></li>
		</ul>
            </td>
          </tr>
        </tbody></table>      
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>I work on <strong>AI for Mathematics</strong>, specifically focusing on:</p>
	      <ul>
		<li>Large language models (LLMs) for formal theorem proving: <a href="#coqgym">[CoqGym]</a>, <a href="#leandojo">[LeanDojo]</a>, <a href="#lips">[LIPS]</a></li>
		<li>Autoformalization: <a href="#leaneuclid">[LeanEuclid]</a></li>
		<li>AI as copilots for mathematicians: <a href="#leancopilot">[Lean Copilot]</a></li>
		<li>LLMs for solving problems in mathematics and sciences: <a href="#sciglm">[SciInstruct]</a></li>
		<li>Test-time compute for reasoning: <a href="#nlproofs">[NLProofS]</a></li>
		<li>Neuro-symbolic machine learning: <a href="#metaqnl">[MetaQNL]</a>, <a href="#lips">[LIPS]</a></li>
	      </ul>
            </td>
          </tr>
		
        </tbody></table>
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	  <tr onmouseout="lips_stop()" onmouseover="lips_start()" id="lips" bgcolor="#ffffd0">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lips_image'>
                  <img src='images/lips-2.png' width="150">
                </div>
                <img src='images/lips-1.png' width="150">
              </div>
              <script type="text/javascript">
                function lips_start() {
                  document.getElementById('lips_image').style.opacity = "1";
                }

                function lips_stop() {
                  document.getElementById('lips_image').style.opacity = "0";
                }
                lips_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://openreview.net/forum?id=FiyS0ecSm0">
                <papertitle>Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning</papertitle>
	      </a>
              <br />
	      <a>Zenan Li</a>*, <a target="_blank" href="https://www.zhaoyu-li.com/">Zhaoyu Li</a>* <a>Wen Tang</a>, <a target="_blank" href="https://www.microsoft.com/en-us/research/people/zhxian/">Xian Zhang</a>, <a>Yuan Yao</a>, <a target="_blank" href="https://www.cs.toronto.edu/~six/">Xujie Si</a>, <a target="_blank" href="https://fanyangcs.github.io/">Fan Yang</a>, <strong>Kaiyu Yang</strong>&dagger;, <a target="_blank" href="https://ics.nju.edu.cn/people/xiaoxingma/">Xiaoxing Ma</a>&dagger; (&dagger; equal advising)
              <br />
              <em>International Conference on Learning Representations (ICLR)</em>, 2025
              <br />
	      <a target="_blank" href="https://openreview.net/forum?id=FiyS0ecSm0">OpenReview</a>
              <p></p>
              <p>To prove inequalities in math competitions, we analyze and distill human techniques into two types of tactics: (1) scaling, handled by symbolic methods, and (2) rewriting, handled by LLMs. The resulting framework combines LLMs with domain-specific mathematical insights, significantly outperforming existing approaches.</p>
            </td>
          </tr>
		
	  <tr id="formal-mathematical-reasoning" bgcolor="#ffffd0">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/formal-mathematical-reasoning.png' width="150">
              </div>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://arxiv.org/abs/2412.16075">
                <papertitle>Formal Mathematical Reasoning: A New Frontier in AI</papertitle>
	      </a>
              <br />
	      <strong>Kaiyu Yang</strong>, <a target="_blank" href="https://gpoesia.com/">Gabriel Poesia</a>, <a target="_blank" href="https://www.sri.inf.ethz.ch/people/jingxuan">Jingxuan He</a>, <a target="_blank" href="https://wenda302.github.io/">Wenda Li</a>, <a target="_blank" href="https://ai.meta.com/people/786716476205590/kristin-e-lauter/">Kristin Lauter</a>, <a target="_blank" href="https://www.cs.utexas.edu/~swarat/">Swarat Chaudhuri</a>, and <a target="_blank" href="https://dawnsong.io/">Dawn Song</a>
              <br />
              <em>Preprint</em>, 2024
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2412.16075">arXiv</a>
              <p></p>
              <p>This position paper advocates for formal mathematical reasoning, i.e., mathematical reasoning grounded in formal systems such as proof assistants. It is complementary to the informal approach (training LLMs on mathematical texts) and is arguably indispensable for advancing AI4Math to the next level.</p>
            </td>
          </tr>
		
	  <tr onmouseout="lean_copilot_stop()" onmouseover="lean_copilot_start()" id="leancopilot">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lean_copilot_image'>
                  <img src='images/lean_copilot-1.png' width="150">
                </div>
                <img src='images/lean_copilot-2.png' width="150">
              </div>
              <script type="text/javascript">
                function lean_copilot_start() {
                  document.getElementById('lean_copilot_image').style.opacity = "1";
                }

                function lean_copilot_stop() {
                  document.getElementById('lean_copilot_image').style.opacity = "0";
                }
                lean_copilot_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://arxiv.org/abs/2404.12534">
                  <papertitle>Towards Large Language Models as Copilots for Theorem Proving in Lean</papertitle>
	      </a>
              <br />
	      <a target="_blank" href="https://peiyang-song.github.io/">Peiyang Song</a>, <strong>Kaiyu Yang</strong>, and <a target="_blank", href="http://tensorlab.cms.caltech.edu/users/anima/index.html">Anima Anandkumar</a>
              <br />
              <em>Preprint</em>, 2024
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2404.12534">arXiv</a>
	       /
              <a target="_blank" href="https://github.com/lean-dojo/LeanCopilot">code</a>
	       /
	      <a target="_blank" href="https://youtu.be/OxFcZU5ihBk">demo</a>
	       /
	      <a target="_blank" href="https://www.youtube.com/watch?v=7NAIXBANSj4">talk</a>
              <p></p>
              <p>
                We introduce a framework for running neural network inference directly in Lean. It enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users, including tools for suggesting proof steps and completing intermediate proof goals using LLMs.
              </p>
            </td>
          </tr>
		
	  <tr id="sciglm">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/sciglm-1.png' width="150">
              </div>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://arxiv.org/abs/2401.07950">
		<papertitle>SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models</papertitle>
	      </a>
              <br />
	      <a target="_blank" href="https://zhangdan0602.github.io/">Dan Zhang</a>, <a target="_blank" href="https://acbull.github.io/">Ziniu Hu</a>, <a>Sining Zhoubian</a>, <a target="_blank" href="https://zxdu.xyz/">Zhengxiao Du</a>, <strong>Kaiyu Yang</strong>, <a>Zihan Wang</a>, <a target="_blank" href="http://www.yisongyue.com/">Yisong Yue</a>, <a target="_blank" href="https://keg.cs.tsinghua.edu.cn/yuxiao/">Yuxiao Dong</a>, and <a target="_blank" href="https://keg.cs.tsinghua.edu.cn/jietang/">Jie Tang</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track</em>, 2024
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2401.07950">arXiv</a>
	       /
	      <a target="_blank" href="https://github.com/THUDM/SciGLM">code</a>
              <p></p>
              <p>
                We curated <em>SciInstruct</em>, a diverse and high-quality dataset of college-level mathematics, physics, chemistry, and formal proofs. Using SciInstruct to finetune the ChatGLM family of LLMs, we introduce <em>SciGLM</em>, a suite of scientific language models for college-level mathematical/scientific reasoning. 
              </p>
            </td>
          </tr>
		
	  <tr id="ntp_survey">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <img src='images/ntp_survey.jpg' width="150">
              </div>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://arxiv.org/abs/2404.09939">
                <papertitle>A Survey on Deep Learning for Theorem Proving</papertitle>
	      </a>
              <br />
	      <a target="_blank" href="https://www.zhaoyu-li.com/">Zhaoyu Li</a>, <a target="_blank" href="https://www.linkedin.com/in/jack-sun-2741711b5/?originalSubdomain=ca">Jialiang Sun</a>, <a target="_blank" href="https://www.cs.toronto.edu/~lmurphy/">Logan Murphy</a>, <a target="_blank" href="https://www.cs.toronto.edu/~qdsu/">Qidong Su</a>, <a target="_blank" href="https://scholar.google.com/citations?user=eu4eqTcAAAAJ&hl=zh-CN">Zenan Li</a>, <a target="_blank" href="https://www.microsoft.com/en-us/research/people/zhxian/">Xian Zhang</a>, <strong>Kaiyu Yang</strong>, and <a target="_blank" href="https://www.cs.toronto.edu/~six/">Xujie Si</a>
              <br />
              <em>Conference on Language Modeling (COLM)</em>, 2024
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2404.09939">arXiv</a>
	       /
	      <a target="_blank" href="https://github.com/zhaoyu-li/DL4TP">code</a>
              <p></p>
              <p>
        	We present the first comprehensive survey of deep learning for theorem proving and autoformalization.
              </p>
            </td>
          </tr>
		
	  <tr onmouseout="lean_euclid_stop()" onmouseover="lean_euclid_start()" bgcolor="#ffffd0" id="leaneuclid">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lean_euclid_image'>
                  <img src='images/lean_euclid-1.png' width="150">
                </div>
                <img src='images/lean_euclid-2.png' width="150">
              </div>
              <script type="text/javascript">
                function lean_euclid_start() {
                  document.getElementById('lean_euclid_image').style.opacity = "1";
                }

                function lean_euclid_stop() {
                  document.getElementById('lean_euclid_image').style.opacity = "0";
                }
                lean_euclid_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
	      <a target="_blank" href="https://arxiv.org/abs/2405.17216">
                <papertitle>Autoformalizing Euclidean Geometry</papertitle>
	      </a>
              <br />
	      <a target="_blank" href="https://www.cs.toronto.edu/~lmurphy/">Logan Murphy</a>*,  <strong>Kaiyu Yang</strong>*, <a target="_blank" href="https://www.linkedin.com/in/jack-sun-2741711b5/?originalSubdomain=ca">Jialiang Sun</a>, <a target="_blank" href="https://www.zhaoyu-li.com/">Zhaoyu Li</a>, <a target="_blank", href="http://tensorlab.cms.caltech.edu/users/anima/index.html">Anima Anandkumar</a>, and <a target="_blank" href="https://www.cs.toronto.edu/~six/">Xujie Si</a> (* equal contribution)
              <br />
              <em>International Conference on Machine Learning (ICML)</em>, 2024
              <br />
	      <a target="_blank" href="https://arxiv.org/abs/2405.17216">arXiv</a>
	       /
              <a target="_blank" href="https://github.com/loganrjmurphy/LeanEuclid">code</a>
              <p></p>
              <p>
        	We release <em>LeanEuclid</em>, a benchmark for testing autoformalization, consisting of Euclid's <em>Elements</em> (Book I) manually formalized in Lean. It is challenging for state-of-the-art LLMs like GPT-4V. Furthermore, the process of constructing LeanEuclid has uncovered intriguing ambiguities in Euclid's original works.
              </p>
            </td>
          </tr>
	
          <tr onmouseout="leandojo_stop()" onmouseover="leandojo_start()" bgcolor="#ffffd0" id="leandojo">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='leandojo_image'>
                  <img src='images/leandojo-1.jpg' width="150">
                </div>
                <img src='images/leandojo-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function leandojo_start() {
                  document.getElementById('leandojo_image').style.opacity = "1";
                }

                function leandojo_stop() {
                  document.getElementById('leandojo_image').style.opacity = "0";
                }
                leandojo_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://leandojo.org/">
                <papertitle>LeanDojo: Theorem Proving with Retrieval-Augmented Language Models</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, <a target="_blank" href="https://aidanswope.com/about">Aidan Swope</a>, <a target="_blank" href="https://minimario.github.io/">Alex Gu</a>, <a target="_blank" href="https://rchalamala.github.io/">Rahul Chalamala</a>, <a target="_blank" href="https://peiyang-song.github.io/">Peiyang Song</a>, <a target="_blank" href="https://billysx.github.io/">Shixing Yu</a>, <a target="_blank" href="https://www.linkedin.com/in/saad-godil-9728353/">Saad Godil</a>, <a target="_blank" href="https://www.linkedin.com/in/ryan-prenger-18797ba1/">Ryan Prenger</a>, and <a target="_blank", href="http://tensorlab.cms.caltech.edu/users/anima/index.html">Anima Anandkumar</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track</em>, 2023, <font color="red"><strong>Oral presentation</strong></font>
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2306.15626">arXiv</a>
	      /
              <a target="_blank" href="https://leandojo.org/">project</a>
	      /
	      <a target="_blank" href="https://github.com/orgs/lean-dojo">code</a>
	      /
	      <a target="_blank" href="https://www.youtube.com/watch?v=qvaR_VXB4fA&feature=youtu.be">talk</a>
	      /
	      <a target="_blank" href="data/LeanDojo_slides.pdf">slides</a>
	      /
	      <a target="_blank" href="https://www.marktechpost.com/2023/07/01/can-llms-generate-mathematical-proofs-that-can-be-rigorously-checked-meet-leandojo-an-open-source-ai-playground-with-toolkits-benchmarks-and-models-for-large-language-models-to-prove-formal-theore/">media</a>
              <p></p>
              <p>Can LLMs generate mathematical proofs that can be rigorously checked? We release <em>LeanDojo</em>: an open-source playground consisting of toolkits, benchmarks, and models for LLMs to prove formal theorems in the Lean proof assistant.</p>
            </td>
          </tr>	
		
          <tr>
            <td style="padding:5px;width:17%;vertical-align:middle">
		<video width="150" controls>
		  <source src="images/infinigen.mp4" type="video/mp4">
		  Your browser does not support HTML video.
		</video>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://infinigen.org/">
                <papertitle>Infinite Photorealistic Worlds using Procedural Generation</papertitle>
              </a>
              <br />
	      <a target="_blank" href="https://araistrick.github.io/">Alexander Raistrick</a>*, <a target="_blank" href=https://www.lahavlipson.com/"">Lahav Lipson</a>*, <a target="_blank" href="https://mazeyu.github.io/">Zeyu Ma</a>*, <a target="_blank" href="https://www.cs.princeton.edu/~lm5483/">Lingjie Mei</a>, <a target="_blank" href="https://www.cs.princeton.edu/~mingzhew/">Mingzhe Wang</a>, <a target="_blank" href="https://zuoym15.github.io/">Yiming Zuo</a>, <a target="_blank" href="https://kkayan.com/">Karhan Kayan</a>, <a target="_blank" href="https://hermera.github.io/">Hongyu Wen</a>, <a target="_blank" href="">Beining Han</a>, <a target="_blank" href="">Yihan Wang</a>, <a target="_blank" href="https://www.alejandronewell.com/">Alejandro Newell</a>, <a target="_blank" href="https://heilaw.github.io/">Hei Law</a>, <a target="_blank" href="https://imankgoyal.github.io/">Ankit Goyal</a>, <strong>Kaiyu Yang</strong>, and <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2306.09310">arXiv</a>
	      /
	      <a target="_blank" href="https://infinigen.org/">project</a>
	      /
              <a target="_blank" href="https://github.com/princeton-vl/infinigen">code</a>
              <p></p>
              <p>
                Data drives progress in computer vision. We introduce <em>Infinigen</em>: a generator of unlimited high-quality 3D data. 100% procedural, no external assets, no AI. Free and open source.
              </p>
            </td>
          </tr>	
		  
          <tr onmouseout="metaqnl_stop()" onmouseover="metaqnl_start()" bgcolor="#ffffd0" id="metaqnl">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='metaqnl_image'>
                  <img src='images/metaqnl-1.jpg' width="150">
                </div>
                <img src='images/metaqnl-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function metaqnl_start() {
                  document.getElementById('metaqnl_image').style.opacity = "1";
                }

                function metaqnl_stop() {
                  document.getElementById('metaqnl_image').style.opacity = "0";
                }
                metaqnl_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2111.12038">
                <papertitle>Learning Symbolic Rules for Reasoning in Quasi-Natural Language</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong> and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Transactions on Machine Learning Research (TMLR)</em>, 2023
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2111.12038">arXiv</a>
	      /
              <a target="_blank" href="https://github.com/princeton-vl/MetaQNL">code</a>
              <p></p>
              <p>
                We propose <em>MetaQNL</em>, a symbolic "Quasi-Natural" language that can express both formal logic and natural language. Instead of manually constructing MetaQNL rules, we propose <em>MetaInduce</em>: an algorithm for learning rules from data. 
              </p>
            </td>
          </tr>		
          
          
          <tr onmouseout="nlproofs_stop()" onmouseover="nlproofs_start()" bgcolor="#ffffd0" id="nlproofs">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nlproofs_image'>
                  <img src='images/nlproofs-1.jpg' width="150">
                </div>
                <img src='images/nlproofs-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function nlproofs_start() {
                  document.getElementById('nlproofs_image').style.opacity = "1";
                }

                function nlproofs_stop() {
                  document.getElementById('nlproofs_image').style.opacity = "0";
                }
                nlproofs_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2205.12443">
                <papertitle>Generating Natural Language Proofs with Verifier-Guided Search</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, and 
							<a target="_blank" href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>
              <br />
              <em>Empirical Methods in Natural Language Processing (EMNLP)</em>, 2022, <font color="red"><strong>Oral presentation</strong></font>
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2205.12443">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-nlp/NLProofS">code</a>
              /
              <a target="_blank" href="data/nlproofs_slides.pdf">slides</a>
              /
              <a target="_blank" href="data/nlproofs_poster.pdf">poster</a>
              <p></p>
              <p>
                We introduce <em>NLProofS</em> (<u>N</u>atural <u>L</u>anguage <u>Proof</u> <u>S</u>earch) for multi-step logical reasoning in natural language. Given a hypothesis and a set of supporting facts, it generates a proof tree indicating how to derive the hypothesis from supporting facts.
              </p>
            </td>
          </tr>

          <tr onmouseout="imagenet_privacy_stop()" onmouseover="imagenet_privacy_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='imagenet_privacy_image'>
                  <img src='images/imagenet_privacy-1.jpg' width="150">
                </div>
                <img src='images/imagenet_privacy-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function imagenet_privacy_start() {
                  document.getElementById('imagenet_privacy_image').style.opacity = "1";
                }

                function imagenet_privacy_stop() {
                  document.getElementById('imagenet_privacy_image').style.opacity = "0";
                }
                imagenet_privacy_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="http://image-net.org/face-obfuscation">
                <papertitle>A Study of Face Obfuscation in ImageNet</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.linkedin.com/in/jacqueline-yau-836b0a132">Jacqueline Yau</a>, 
              <a target="_blank" href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, and 
							<a target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a> 
              <br />
              <em>International Conference on Machine Learning (ICML)</em>, 2022
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2103.06191">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princetonvisualai/imagenet-face-obfuscation">code</a>
              /
              <a target="_blank" href="data/imagenet_privacy_slides.pdf">slides</a>
              /
              <a target="_blank" href="https://youtu.be/VnszYe8abRs?t=889">talk</a>
              /
              <a target="_blank" href="data/imagenet_face_obfuscation_icml_poster.pdf">poster</a>
              /
              <a target="_blank" href="http://image-net.org/face-obfuscation">project</a>
              /
              <a target="_blank" href="https://insights.princeton.edu/2022/01/tradeoff-privacy-algorithm-performance">media</a>
              <p></p>
              <p>We annotate human faces in ImageNet and obfuscate them for privacy protection. We show that face obfuscation does not hurt image classification and transfer learning.</p>
            </td>
          </tr>

          <tr onmouseout="attach_juxtapose_stop()" onmouseover="attach_juxtapose_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='attach_juxtapose_image'>
                  <img src='images/attach_juxtapose-1.jpg' width="150">
                </div>
                <img src='images/attach_juxtapose-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function attach_juxtapose_start() {
                  document.getElementById('attach_juxtapose_image').style.opacity = "1";
                }

                function attach_juxtapose_stop() {
                  document.getElementById('attach_juxtapose_image').style.opacity = "0";
                }
                attach_juxtapose_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2010.14568">
                <papertitle>Strongly Incremental Constituency Parsing with Graph Neural Networks</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong> and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2020
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2010.14568">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/attach-juxtapose-parser">code</a>
              /
              <a target="_blank" href="data/parsing_neurips_slides.pdf">slides</a>
              /
              <a target="_blank" href="https://crossminds.ai/video/strongly-incremental-constituency-parsing-with-graph-neural-networks-606fee03f43a7f2f827c13c3/">talk</a>
              /
              <a target="_blank" href="data/parsing_neurips_poster.pdf">poster</a>
              <p></p>
              <p>We propose a novel transition-based constituency parser named <em>Attach-Juxtapose</em>, inspired by how humans perform parsing.</p>
            </td>
          </tr>
					
          <tr>
            <td style="padding:5px;width:17%;vertical-align:middle">
              <img src='images/rel3d.gif' width="150">
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2012.01634">
                <papertitle>Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://imankgoyal.github.io/">Ankit Goyal</a>, 
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="http://www-personal.umich.edu/~ydawei/">Dawei Yang</a>, and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2020, <font color="red"><strong>Spotlight presentation</strong></font>
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2012.01634">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/Rel3D">code</a>
              <p></p>
              <p>We propose <em>Minimally Contrastive Data Collection</em>: a novel crowdsourcing method for reducing dataset bias. And we use it to construct Rel3D—the first large-scale, human-annotated dataset for grounding spatial relations in 3D.</p>
            </td>
          </tr>

          <tr onmouseout="imagenet_fairness_stop()" onmouseover="imagenet_fairness_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='imagenet_fairness_image'>
                  <img src='images/imagenet_fairness-1.jpg' width="150">
                </div>  
                <img src='images/imagenet_fairness-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function imagenet_fairness_start() {
                  document.getElementById('imagenet_fairness_image').style.opacity = "1";
                }

                function imagenet_fairness_stop() {
                  document.getElementById('imagenet_fairness_image').style.opacity = "0";
                }
                imagenet_fairness_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1912.07726">
                <papertitle>Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.linkedin.com/in/klint-qinami/">Klint Qinami</a>, 
              <a target="_blank" href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, and 
							<a target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a>
              <br />
              <em>Conference on Fairness, Accountability, and Transparency (FAT*)</em>, 2020
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1912.07726">arXiv</a>
              /
              <a target="_blank" href="data/imagenet_debias_fat_slides.pdf">slides</a>
              /
              <a target="_blank" href="https://www.youtube.com/watch?v=MLaqxuNtgAo">talk</a>
              /
              <a target="_blank" href="http://image-net.org/update-sep-17-2019">blog</a>
              /
              <a target="_blank" href="https://www.wired.com/story/ai-biased-how-scientists-trying-fix">media</a>
              <p></p>
              <p>We reveal and mitigate fairness issues of ImageNet, filtering its concept vocabulary and balancing its representation of various demographic groups in images.</p>
            </td>
          </tr>

          <tr onmouseout="coqgym_stop()" onmouseover="coqgym_start()"  bgcolor="#ffffd0" id="coqgym">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='coqgym_image'>
                  <img src='images/coqgym-1.jpg' width="150">
                </div> 
                <img src='images/coqgym-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function coqgym_start() {
                  document.getElementById('coqgym_image').style.opacity = "1";
                }

                function coqgym_stop() {
                  document.getElementById('coqgym_image').style.opacity = "0";
                }
                coqgym_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1905.09381">
                <papertitle>Learning to Prove Theorems via Interacting with Proof Assistants</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong> and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>International Conference on Machine Learning (ICML)</em>, 2019
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1905.09381">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/CoqGym">code</a>
              /
              <a target="_blank" href="data/coq_icml_slides.pdf">slides</a>
              /
              <a target="_blank" href="data/coq_icml_poster.pdf">poster</a>
              <p></p>
              <p>We introduce CoqGym, one of the first and largest datasets for theorem proving in proof assistants, and ASTactic, a deep learning prover generating tactics as programs. </p>
            </td>
          </tr>

          <tr onmouseout="spatialsense_stop()" onmouseover="spatialsense_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='spatialsense_image'>
                  <img src='images/spatialsense-1.jpg' width="150">
                </div>
                <img src='images/spatialsense-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function spatialsense_start() {
                  document.getElementById('spatialsense_image').style.opacity = "1";
                }

                function spatialsense_stop() {
                  document.getElementById('spatialsense_image').style.opacity = "0";
                }
                spatialsense_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1908.02660">
                <papertitle>SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a>, and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>International Conference on Computer Vision (ICCV)</em>, 2019
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1908.02660">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/SpatialSense">code</a>
              /
              <a target="_blank" href="data/spatialsense_iccv_poster.pdf">poster</a>
              <p></p>
              <p>We propose <em>Adversarial Crowdsourcing</em> to reduce dataset bias and use it to construct SpatialSense, a challenging dataset for recognizing spatial relations in images.</p>
            </td>
          </tr>

          <tr onmouseout="hourglass_stop()" onmouseover="hourglass_start()">
            <td style="padding:5px;width:17%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hourglass_image'>
                  <img src='images/hourglass-1.jpg' width="150">
                </div>
                <img src='images/hourglass-2.jpg' width="150">
              </div>
              <script type="text/javascript">
                function hourglass_start() {
                  document.getElementById('hourglass_image').style.opacity = "1";
                }

                function hourglass_stop() {
                  document.getElementById('hourglass_image').style.opacity = "0";
                }
                hourglass_stop()
              </script>
            </td>
            <td style="padding:20px;width:83%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1603.06937">
                <papertitle>Stacked Hourglass Networks for Human Pose Estimation</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://www.alejandronewell.com/">Alejandro Newell</a>,
              <strong>Kaiyu Yang</strong>, and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>European Conference on Computer Vision (ECCV)</em>, 2016
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1603.06937">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/pose-hg-train">code</a>
              <p></p>
              <p>We introduce <em>Stacked Hourglass Networks</em>—one of the most popular architectures for human pose estimation, object detection, and more.</p>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Workshops & Tutorials</heading>
		<p>I'm a co-organizer of the following events:</p>
		<ul>
		  <li><a target="_blank" href="https://mathai2023.github.io/">The 3rd Workshop on Mathematical Reasoning and AI</a> @ NeurIPS 2023 (<a target="_blank" href="https://neurips.cc/virtual/2023/workshop/66522">Video</a>)</li>
		  <li><a target="_blank" href="https://machine-learning-for-theorem-proving.github.io/">Tutorial on Machine Learning for Theorem Proving</a> @ NeurIPS 2023 (<a target="_blank" href="https://neurips.cc/virtual/2023/tutorial/73946">Video</a>)</li>
		</ul>
            </td>
          </tr>
        </tbody></table>
	      
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Media</heading>
	      <p>My works are covered by:</p>
		<ul>
		  <li><a target="_blank" href="https://www.marktechpost.com/2023/07/01/can-llms-generate-mathematical-proofs-that-can-be-rigorously-checked-meet-leandojo-an-open-source-ai-playground-with-toolkits-benchmarks-and-models-for-large-language-models-to-prove-formal-theore/">Can LLMs Generate Mathematical Proofs that can be Rigorously Checked?</a>, MarkTechPost, 2023</li>
		  <li><a target="_blank" href="https://insights.princeton.edu/2022/01/tradeoff-privacy-algorithm-performance/">Exploring the Tradeoff Between Privacy and Algorithm Performance</a>, Princeton Insights, 2022</li>
		  <li><a target="_blank" href="https://engineering.princeton.edu/news/2020/02/12/researchers-devise-approach-reduce-biases-computer-vision-data-sets">Researchers Devise Approach to Reduce Biases in Computer Vision Data Sets</a>, Princeton Engineering News, 2020</li>
		  <li><a target="_blank" href="https://www.wired.com/story/ai-biased-how-scientists-trying-fix">AI Is Biased. Here's How Scientists Are Trying to Fix It</a>, Wired, 2019</li>
		</ul>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading id="mentoring">Mentoring</heading>
		<p>I'm fortunate to have worked with many talented students:</p>
		<ul>
		  <li><a target="_blank" href="https://www.zhaoyu-li.com/">Zhaoyu Li</a>: Ph.D. student @ University of Toronto</li>
		  <li><a target="_blank" href="https://jc-chen1.github.io/">Jiacheng Chen</a>: Undergrad @ South China University of Technology</li>
		  <li><a target="_blank" href="https://peiyang-song.github.io/">Peiyang Song</a>: Undergrad @ UCSB -> Undergrad @ Caltech</li>
		  <li><a target="_blank" href="https://chalamala.com/">Rahul Chalamala</a>: Undergrad @ Caltech -> Researcher @ Together AI</li>
		  <li><a target="_blank" href="https://billysx.github.io/">Shixing Yu</a>: Master's student @ UT Austin -> Ph.D. student @ Cornell</li>
		  <li><a target="_blank" href="https://genechou.com/">Gene Chou</a>: Undergrad @ Princeton -> Ph.D. student @ Cornell </li>
		  <li><a target="_blank" href="https://www.linkedin.com/in/jacqueline-yau-836b0a132/">Jacqueline Yau</a>: Master’s student @ Stanford -> Ph.D. student @ UIUC </li>
		</ul>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
		<ul>
		  <li>Guest Lecturer, <a target="_blank" href="https://sites.google.com/view/cs-159-2024/home">CS 159: Large Language Models for Reasoning</a>, Caltech</li>
		  <li>Teaching Assistant, <a target="_blank" href="https://nlp.cs.princeton.edu/cos484-sp21/">COS 485/585: Natural Language Processing</a>, Princeton University</li>
		  <li>Head Teaching Assistant, <a target="_blank" href="https://dsa.cs.tsinghua.edu.cn/~deng/ds/index.htm">Data Structures and Algorithms</a>, Tsinghua University</li>
		</ul>
            </td>
          </tr>
        </tbody></table>

	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br />
              <p style="text-align:right;font-size:small;">
                Website template credit: <a target="_blank" href="https://jonbarron.info/" style="text-align:right;font-size:small;">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

</body>

</html>
