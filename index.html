<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kaiyu Yang</title>
  
  <meta name="author" content="Kaiyu Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
  <link rel="manifest" href="images/site.webmanifest">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kaiyu Yang</name>
              </p>
              <p> 
                I am a postdoctoral researcher at Caltech in the <a style="text-decoration: none" target="_blank" href="https://www.cms.caltech.edu/">Computing + Mathematical Sciences (CMS) Department</a>
                , working with <a style="text-decoration: none" target="_blank" href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>. 
                I received my Ph.D. from Princeton University, where I was advised by <a style="text-decoration: none" target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a> 
                and also worked with <a style="text-decoration: none" target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a> and <a style="text-decoration: none" target="_blank" href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>. 
              </p>
              <p style="text-align:center">
                kaiyuy (at) caltech.edu &nbsp/&nbsp
                <a target="_blank" href="data/CV___Kaiyu_Yang.pdf">CV</a> &nbsp/&nbsp
                <a target="_blank" href="data/Bio.txt">Bio</a> &nbsp/&nbsp
                <a target="_blank"href="https://scholar.google.com/citations?user=FciCu4EAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://github.com/yangky11">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Kaiyu Yang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Kaiyu Yang - Circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research aims to make machine learning capable of symbolic reasoning. I have approached the goal from two angles: (1) applying machine learning to symbolic reasoning tasks, such as automated theorem proving; (2) introducing symbolic components into machine learning models to make them more interpretable, verifiable, and data-efficient. 
              </p>
              <p>
                In addition, I also work on constructing and analyzing machine learning datasets, focusing on fairness, privacy, and mitigating dataset bias. They collectively contribute to a future of machine learning we can trust, even in real-world, high-stake applications.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
		  
          <tr onmouseout="metaqnl_stop()" onmouseover="metaqnl_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='metaqnl_image'>
                  <img src='images/metaqnl-1.jpg' width="160"></div>
                  <img src='images/metaqnl-2.jpg' width="160">
                </div>
              <script type="text/javascript">
                function metaqnl_start() {
                  document.getElementById('metaqnl_image').style.opacity = "1";
                }

                function metaqnl_stop() {
                  document.getElementById('metaqnl_image').style.opacity = "0";
                }
                metaqnl_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2111.12038">
                <papertitle>Learning Symbolic Rules for Reasoning in Quasi-Natural Language</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong> and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Under review</em>, 2022
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2111.12038">arXiv</a>
              <p></p>
              <p>
                We propose <em>MetaQNL</em>, a symbolic "Quasi-Natural" language that can express both formal logic and natural language, and <em>MetaInduce</em>, an algorithm for learning MetaQNL rules from data. 
              </p>
            </td>
          </tr>		
          
          
          <tr onmouseout="nlproofs_stop()" onmouseover="nlproofs_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nlproofs_image'></div>
                  <img src='images/nlproofs.jpg' width="160">
                  <img src='images/nlproofs.jpg' width="160">
                </div>
              <script type="text/javascript">
                function nlproofs_start() {
                  document.getElementById('nlproofs_image').style.opacity = "1";
                }

                function nlproofs_stop() {
                  document.getElementById('nlproofs_image').style.opacity = "0";
                }
                nlproofs_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2205.12443">
                <papertitle>Generating Natural Language Proofs with Verifier-Guided Search</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, and 
							<a target="_blank" href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>
              <br />
              <em>Empirical Methods in Natural Language Processing (EMNLP)</em>, 2022, Oral presentation
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2205.12443">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-nlp/NLProofS">code</a>
              /
              <a target="_blank" href="data/nlproofs_slides.pdf">slides</a>
              /
              <a target="_blank" href="data/nlproofs_poster.pdf">poster</a>
              <p></p>
              <p>
                We introduce NLProofS (<u>N</u>atural <u>L</u>anguage <u>P</u>roof <u>S</u>earch) for multi-step logical reasoning in natural language. Given a hypothesis and a set of supporting facts in natural language, the model generates a proof tree indicating how to derive the hypothesis from supporting facts.
              </p>
            </td>
          </tr>

          <tr onmouseout="imagenet_privacy_stop()" onmouseover="imagenet_privacy_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='imagenet_privacy_image'></div>
                  <img src='images/imagenet_privacy.jpg' width="160">
                  <img src='images/imagenet_privacy.jpg' width="160">
                </div>
              <script type="text/javascript">
                function imagenet_privacy_start() {
                  document.getElementById('imagenet_privacy_image').style.opacity = "1";
                }

                function imagenet_privacy_stop() {
                  document.getElementById('imagenet_privacy_image').style.opacity = "0";
                }
                imagenet_privacy_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="http://image-net.org/face-obfuscation">
                <papertitle>A Study of Face Obfuscation in ImageNet</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.linkedin.com/in/jacqueline-yau-836b0a132">Jacqueline Yau</a>, 
              <a target="_blank" href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, and 
							<a target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a> 
              <br />
              <em>International Conference on Machine Learning (ICML)</em>, 2022
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2103.06191">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princetonvisualai/imagenet-face-obfuscation">code</a>
              /
              <a target="_blank" href="data/imagenet_privacy_slides.pdf">slides</a>
              /
              <a target="_blank" href="https://youtu.be/VnszYe8abRs?t=889">talk</a>
              /
              <a target="_blank" href="data/imagenet_face_obfuscation_icml_poster.pdf">poster</a>
              /
              <a target="_blank" href="http://image-net.org/face-obfuscation">project</a>
              /
              <a target="_blank" href="https://insights.princeton.edu/2022/01/tradeoff-privacy-algorithm-performance">media</a>
              <p></p>
              <p>We annotate human faces in ImageNet and obfuscate them for privacy protection. Our experiments show that face obfuscation does not hurt image classification and transfer learning.</p>
            </td>
          </tr>

          <tr onmouseout="attach_juxtapose_stop()" onmouseover="attach_juxtapose_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='attach_juxtapose_image'></div>
                  <img src='images/attach_juxtapose.jpg' width="160">
                  <img src='images/attach_juxtapose.jpg' width="160">
                </div>
              <script type="text/javascript">
                function attach_juxtapose_start() {
                  document.getElementById('attach_juxtapose_image').style.opacity = "1";
                }

                function attach_juxtapose_stop() {
                  document.getElementById('attach_juxtapose_image').style.opacity = "0";
                }
                attach_juxtapose_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2010.14568">
                <papertitle>Strongly Incremental Constituency Parsing with Graph Neural Networks</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong> and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2020
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2010.14568">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/attach-juxtapose-parser">code</a>
              /
              <a target="_blank" href="data/parsing_neurips_slides.pdf">slides</a>
              /
              <a target="_blank" href="https://crossminds.ai/video/strongly-incremental-constituency-parsing-with-graph-neural-networks-606fee03f43a7f2f827c13c3/">talk</a>
              /
              <a target="_blank" href="data/parsing_neurips_poster.pdf">poster</a>
              <p></p>
              <p>
                Psycholinguistic research suggests that human parsing is strongly incremental&mdash;humans grow a single parse tree by adding exactly one token at each step. We propose a strongly incremental transition system for parsing named *Attach-Juxtapose*. It represents a partial sentence using a single tree, and each action adds exactly one token into the partial tree. Based on our transition system, we develop a strongly incremental parser that achieves state of the art on Penn Treebank and Chinese Treebank.
              </p>
            </td>
          </tr>
					
          <tr onmouseout="rel3d_stop()" onmouseover="rel3d_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rel3d_image'></div>
                  <img src='images/rel3d.jpg' width="160">
                  <img src='images/rel3d.jpg' width="160">
                </div>
              <script type="text/javascript">
                function rel3d_start() {
                  document.getElementById('nlproofs_image').style.opacity = "1";
                }

                function rel3d_stop() {
                  document.getElementById('rel3d_image').style.opacity = "0";
                }
                rel3d_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/2012.01634">
                <papertitle>Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D</papertitle>
              </a>
              <br />
              <a target="_blank" href="https://imankgoyal.github.io/">Ankit Goyal</a>, 
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="http://www-personal.umich.edu/~ydawei/">Dawei Yang</a>, and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2020, Spotlight presentation
              <br />
              <a target="_blank" href="https://arxiv.org/abs/2012.01634">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/Rel3D">code</a>
              <p></p>
              <p>
                We construct Rel3D: the first large-scale, human-annotated dataset for grounding spatial relations in 3D. It enables quantifying the effectiveness of 3D information in predicting spatial relations. Moreover, we propose minimally contrastive data collection&mdash;a novel crowdsourcing method for reducing dataset bias. The examples in Rel3D come in minimally contrastive pairs: two examples in a pair are almost identical but have different labels.
              </p>
            </td>
          </tr>

          <tr onmouseout="imagenet_fairness_stop()" onmouseover="imagenet_fairness_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='imagenet_fairness_image'></div>
                  <img src='images/imagenet_fairness.jpg' width="160">
                  <img src='images/imagenet_fairness.jpg' width="160">
                </div>
              <script type="text/javascript">
                function imagenet_fairness_start() {
                  document.getElementById('imagenet_fairness_image').style.opacity = "1";
                }

                function imagenet_fairness_stop() {
                  document.getElementById('imagenet_fairness_image').style.opacity = "0";
                }
                imagenet_fairness_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1912.07726">
                <papertitle>Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.linkedin.com/in/klint-qinami/">Klint Qinami</a>, 
              <a target="_blank" href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, and 
							<a target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a>
              <br />
              <em>Conference on Fairness, Accountability, and Transparency (FAT*)</em>, 2020
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1912.07726">arXiv</a>
              /
              <a target="_blank" href="data/imagenet_debias_fat_slides.pdf">slides</a>
              /
              <a target="_blank" href="https://www.youtube.com/watch?v=MLaqxuNtgAo">talk</a>
              /
              <a target="_blank" href="http://image-net.org/update-sep-17-2019">blog</a>
              /
              <a target="_blank" href="https://www.wired.com/story/ai-biased-how-scientists-trying-fix">media</a>
              <p></p>
              <p>
                Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior.  

In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the <code>person</code> subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.
              </p>
            </td>
          </tr>

          <tr onmouseout="coqgym_stop()" onmouseover="coqgym_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='coqgym_image'></div>
                  <img src='images/coqgym.jpg' width="160">
                  <img src='images/coqgym.jpg' width="160">
                </div>
              <script type="text/javascript">
                function coqgym_start() {
                  document.getElementById('nlproofs_image').style.opacity = "1";
                }

                function coqgym_stop() {
                  document.getElementById('nlproofs_image').style.opacity = "0";
                }
                coqgym_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1905.09381">
                <papertitle>Learning to Prove Theorems via Interacting with Proof Assistants</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong> and 
              <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>International Conference on Machine Learning (ICML)</em>, 2019
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1905.09381">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/CoqGym">code</a>
              /
              <a target="_blank" href="data/coq_icml_slides.pdf">slides</a>
              /
              <a target="_blank" href="data/coq_icml_poster.pdf">poster</a>
              <p></p>
              <p>
                We use machine learning to automatically prove theorems, including not only theorems in math but also theorems describing the behavior of software and hardware systems.
Current theorem provers usually search for proofs represented at a low level, such as first-order logic and resolutions. Therefore they lack the high-level reasoning and problem-specific insights common to humans.

In contrast, we use a powerful set of tools called proof assistants (a.k.a. interactive theorem provers). These are software that assists human experts in proving theorems. They thus provide a high-level framework that is close to human mathematical reasoning. Instead of humans, we develop machine learning agents to interact with proof assistants. Our agent can learn from human interactions by imitation learning using a large amount of data available online. We use this data to construct a large-scale dataset for training/evaluating the agent. We also develop a baseline model that can prove many new theorems not provable by existing methods.
              </p>
            </td>
          </tr>

          <tr onmouseout="spatialsense_stop()" onmouseover="spatialsense_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='spatialsense_image'></div>
                  <img src='images/spatialsense.jpg' width="160">
                  <img src='images/spatialsense.jpg' width="160">
                </div>
              <script type="text/javascript">
                function spatialsense_start() {
                  document.getElementById('spatialsense_image').style.opacity = "1";
                }

                function spatialsense_stop() {
                  document.getElementById('spatialsense_image').style.opacity = "0";
                }
                spatialsense_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1908.02660">
                <papertitle>SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition</papertitle>
              </a>
              <br />
              <strong>Kaiyu Yang</strong>, 
              <a target="_blank" href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a>, and 
              <a target="_blank" href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>International Conference on Computer Vision (ICCV)</em>, 2019
              <br />
              <a target="_blank" href="https://arxiv.org/abs/1908.02660">arXiv</a>
              /
              <a target="_blank" href="https://github.com/princeton-vl/SpatialSense">code</a>
              /
              <a target="_blank" href="data/spatialsense_iccv_poster.pdf">poster</a>
              <p></p>
              <p>
                Benchmarks in vision and language suffer from dataset bias---models can perform exceptionally well by exploiting simple cues without even looking at the image, which undermines the benchmark's value in measuring visual reasoning abilities. We propose adversarial crowdsourcing to reduce dataset bias. Annotators are explicitly tasked with finding examples that are difficult to predict using simple cues such as 2D spatial configuration or language priors. Specifically, we introduce SpatialSense, a challenging dataset for spatial relation recognition collected via adversarial crowdsourcing.
              </p>
            </td>
          </tr>

          <tr onmouseout="hourglass_stop()" onmouseover="hourglass_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hourglass_image'></div>
                  <img src='images/hourglass.jpg' width="160">
                  <img src='images/hourglass.jpg' width="160">
                </div>
              <script type="text/javascript">
                function hourglass_start() {
                  document.getElementById('hourglass_image').style.opacity = "1";
                }

                function hourglass_stop() {
                  document.getElementById('hourglass_image').style.opacity = "0";
                }
                hourglass_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://arxiv.org/abs/1603.06937">
                <papertitle>Stacked Hourglass Networks for Human Pose Estimation</papertitle>
              </a>
              <br />
              <a href="https://www.alejandronewell.com/">Alejandro Newell</a>,
              <strong>Kaiyu Yang</strong>, and 
              <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br />
              <em>European Conference on Computer Vision (ECCV)</em>, 2016
              <br />
              <a href="https://arxiv.org/abs/1603.06937">arXiv</a>
              /
              <a href="https://github.com/princeton-vl/pose-hg-train">code</a>
              <p></p>
              <p>
                We introduce the hourglass network: a novel convolutional network architecture for human pose estimation. It has become a standard component in many state-of-the-art methods for pose estimation.
              </p>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <ul>
          <li><a target="_blank" href="https://princeton-nlp.github.io/cos484/">COS 485/585: Natural Language Processing</a>, Princeton University</li>
          <li><a target="_blank" href="https://dsa.cs.tsinghua.edu.cn/~deng/ds/index.htm">Data Structures and Algorithms</a>, Tsinghua University</li>
        </ul>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br />
              <p style="text-align:right;font-size:small;">
                Website template credit: <a target="_blank" href="https://jonbarron.info/">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

</body>

</html>
